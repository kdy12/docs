## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ 

1. äº†è§£æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ
2. æŒæ¡æ¨¡å‹è®­ç»ƒ/å¾®è°ƒ/å°å‚æ•°é‡å¾®è°ƒçš„æ“ä½œè¿‡ç¨‹
3. æŒæ¡æ¨¡å‹å¾®è°ƒ/å°å‚æ•°é‡å¾®è°ƒå…³é”®ã€Œè¶…å‚ã€
4. æŒæ¡è®­ç»ƒæ•°æ®çš„é€‰æ‹©ã€å‡†å¤‡ã€æ¸…æ´—ç­‰æ–¹æ³•ä¸æ€è·¯
5. è®­ç»ƒä¸€ä¸ªå‚ç›´é¢†åŸŸçš„å¤§æ¨¡å‹

å¼€å§‹ä¸Šè¯¾ï¼


## è¿˜æ˜¯å†™åœ¨å‰é¢

1. è¿™å ‚è¯¾å†…å®¹æœ‰éš¾åº¦ï¼š
   1. æœ‰å¾ˆå¤šé™Œç”Ÿçš„åè¯ï¼ŒåŒ…æ‹¬æ•°å­¦åè¯å’Œæ¨¡å‹ç®—æ³•æœ¬èº«çš„åè¯
   2. æ¶‰åŠåˆ°å¾ˆå¤šæ•°å­¦çŸ¥è¯†ï¼Œå¾ˆå¤šä¸œè¥¿æœ¬èº«æ˜¯ä»æ•°å­¦æ¨å¯¼å‡ºæ¥çš„ï¼Œä¸å¥½å…·è±¡åŒ–
   3. æ·±åº¦å­¦ä¹ é‡Œæœ‰å¤§é‡åŸºäºç»éªŒçš„æ€»ç»“ï¼Œä½“ç°æˆå„ç§è¶…å‚å’Œ Tricks
2. è¿™å ‚è¯¾è¯¥æ€ä¹ˆå­¦ï¼š
   1. æ³¨æ„åŠ›é›†ä¸­ï¼Œè·Ÿä¸Šæˆ‘çš„æ€è·¯
   2. é‡åˆ°ä¸æ‡‚çš„åœ°æ–¹ï¼Œåˆ«å®³æ€•ï¼Œå…ˆå°è¯•æ€è€ƒ
   3. å®ç°æƒ³ä¸æ˜ç™½ä¹Ÿåˆ«ç°å¿ƒï¼Œè¿™ä¸ªé¢†åŸŸçš„èƒ½åŠ›ç§¯ç´¯æ˜¯éœ€è¦æ—¶é—´çš„
   4. çœŸæ„Ÿå…´è¶£çš„åŒå­¦ï¼Œå°è¯•å¤šåº¦è®ºæ–‡ï¼Œâ€œä¹¦è¯»ç™¾éå…¶ä¹‰è‡ªè§â€çš„é“ç†æˆ‘äº²è‡ªéªŒè¯è¿‡
3. **æ·±åº¦å­¦ä¹ æ˜¯åŸºäºæ•°å­¦çš„ç»éªŒç§‘å­¦**
   1. ç¨‹åºå‘˜æ€ç»´ï¼šIF...ELSE... *â€œä½ å°±å‘Šè¯‰æˆ‘é‡åˆ°è¿™ä¸ªæƒ…å†µæ€ä¹ˆåŠâ€*
   2. ç®—æ³•å·¥ç¨‹å¸ˆæ€ç»´ï¼š$P(åŠæ³•|æƒ…å†µ)$ *â€œè¿™ä¸ªæƒ…å†µ**å¤§æ¦‚ç‡**ä½ å¯ä»¥**å°è¯•**XXXâ€*


<div class="alert alert-info">
<b>é¢å‘åˆå­¦è€…çš„æ·±åº¦å­¦ä¹ è¯¾ï¼š</b> 
<ol>
<li>å´æ©è¾¾ã€Šäººäºº AIã€‹(ç‰¹åˆ«é€šä¿—) https://www.zhihu.com/education/video-course/1556316449043668992</li>
<li>ææ²çš„æ·±åº¦å­¦ä¹ è¯¾ (ç¨å¾®æ·±ä¸€ç‚¹) https://www.zhihu.com/education/video-course/1647604835598092705</li>
</ol>
åœ¨è¿™ä¸ªæ›´å¹¿æ³›çš„å®šä½ä¸Šï¼Œå·²ç»æœ‰å¾ˆå¤šä¼˜ç§€çš„è¯¾ç¨‹ã€‚æœ¬è¯¾ç¨‹åªé’ˆå¯¹å¤§æ¨¡å‹å¾®è°ƒçš„çŸ¥è¯†åŸºç¡€å±•å¼€ã€‚
</div>

## ä»€ä¹ˆæ—¶å€™éœ€è¦ Fine-Tuning

1. æœ‰ç§æœ‰éƒ¨ç½²çš„éœ€æ±‚
2. å¼€æºæ¨¡å‹åŸç”Ÿçš„èƒ½åŠ›ä¸æ»¡è¶³ä¸šåŠ¡éœ€æ±‚

## å…ˆçœ‹ä¸€ä¸ªä¾‹å­

http://localhost:6006/

**è®¢é…’åº—æœºå™¨äºº**

```json
[
    {
        "role": "user",
        "content": "æ‚¨å¥½ï¼Œæˆ‘è¦æ‰¾ä¸€å®¶èˆ’é€‚å‹é…’åº—ä½å®¿ï¼Œç„¶åå¸Œæœ›é…’åº—æä¾›æš–æ°”ä¸è¡Œæå¯„å­˜ã€‚"
    },
    {
        "role": "search",
        "arguments": {
            "facilities": [
                "æš–æ°”",
                "è¡Œæå¯„å­˜"
            ],
            "type": "èˆ’é€‚å‹"
        }
    },
    {
        "role": "return",
        "records": [
            {
                "name": "åŒ—äº¬é¦™æ±Ÿæˆ´æ–¯é…’åº—",
                "type": "èˆ’é€‚å‹",
                "address": "åŒ—äº¬ä¸œåŸåŒºå—æ²³æ²¿å¤§è¡—å—æ¹¾å­èƒ¡åŒ1å·",
                "subway": "å¤©å®‰é—¨ä¸œåœ°é“ç«™Bå£",
                "phone": "010-65127788",
                "facilities": [
                    "é…’åº—å„å¤„æä¾›wifi",
                    "å›½é™…é•¿é€”ç”µè¯",
                    "å¹é£æœº",
                    "24å°æ—¶çƒ­æ°´",
                    "æš–æ°”",
                    "è¥¿å¼é¤å…",
                    "ä¸­å¼é¤å…",
                    "æ®‹ç–¾äººè®¾æ–½",
                    "ä¼šè®®å®¤",
                    "æ— çƒŸæˆ¿",
                    "å•†åŠ¡ä¸­å¿ƒ",
                    "æ—©é¤æœåŠ¡",
                    "æ¥æœºæœåŠ¡",
                    "æ¥å¾…å¤–å®¾",
                    "æ´—è¡£æœåŠ¡",
                    "è¡Œæå¯„å­˜",
                    "ç§Ÿè½¦",
                    "å«é†’æœåŠ¡"
                ],
                "price": 328.0,
                "rating": 4.2,
                "hotel_id": 10
            }
        ]
    },
    {
        "role": "assistant",
        "content": "é‚£æ¨èæ‚¨åŒ—äº¬é¦™æ±Ÿæˆ´æ–¯é…’åº—ï¼Œç¬¦åˆæ‚¨çš„å…¨éƒ¨ä½å®¿è¦æ±‚å¹¶ä¸”ä»·æ ¼åˆæ¯”è¾ƒåˆé€‚çš„é…’åº—ã€‚"
    },
    {
        "role": "user",
        "content": "è¿™å®¶é…’åº—çš„ä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ"
    },
    {
        "role": "assistant",
        "content": "è¿™å®¶é…’åº—çš„ä»·æ ¼æ˜¯æ¯æ™š328.0å…ƒã€‚"
    },
    {
        "role": "user",
        "content": "å¥½çš„ï¼Œé‚£å°±é¢„è®¢åŒ—äº¬é¦™æ±Ÿæˆ´æ–¯é…’åº—å§ï¼"
    },
    {
       "role": "assistant",
        "content": "å¥½çš„ï¼Œç¥æ‚¨å…¥ä½æ„‰å¿«ï¼"
    }
]

```

## ä¸€ã€å…ˆæ‰¾æ‰¾æ„Ÿè§‰

ä¸Šæ‰‹æ“ä½œä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼š

- **æƒ…æ„Ÿåˆ†ç±»**
  - è¾“å…¥ï¼šç”µå½±è¯„è®º
  - è¾“å‡ºï¼š\['è´Ÿé¢','æ­£é¢'\]
  - æ•°æ®æºï¼šhttps://huggingface.co/datasets/rotten_tomatoes

### 1.1ã€**å·¥å…·**ï¼šä»‹ç»ä¸€ä¸ªæ¨¡å‹è®­ç»ƒåˆ©å™¨ Hugging Face

- å®˜ç½‘ï¼šhttp://www.huggingface.co
- ç›¸å½“äºé¢å‘ NLP æ¨¡å‹çš„ Github
- å°¤å…¶åŸºäº transformer çš„å¼€æºæ¨¡å‹éå¸¸å…¨
- å°è£…äº†æ¨¡å‹ã€æ•°æ®é›†ã€è®­ç»ƒå™¨ç­‰ï¼Œä½¿æ¨¡å‹çš„ä¸‹è½½ã€ä½¿ç”¨ã€è®­ç»ƒéƒ½éå¸¸æ–¹ä¾¿

**å®‰è£…ä¾èµ–**

```python
# pipå®‰è£…
pip install transformers # å®‰è£…æœ€æ–°çš„ç‰ˆæœ¬
pip install transformers == 4.30 # å®‰è£…æŒ‡å®šç‰ˆæœ¬
# condaå®‰è£…
conda install -c huggingface transformers  # åª4.0ä»¥åçš„ç‰ˆæœ¬
```


### 1.2ã€æ“ä½œæµç¨‹

<br />
<img src="_images/llm/training_process.png" style="margin-left: 0px" width="600px">
<br />

<div class="alert alert-warning">
<b>æ³¨æ„ï¼š</b> 
<ul>
<li>ä»¥ä¸‹çš„ä»£ç ï¼Œéƒ½ä¸è¦åœ¨Jupyterç¬”è®°ä¸Šç›´æ¥è¿è¡Œï¼Œä¼šæ­»æœºï¼ï¼</li>
<li>è¯·ä¸‹è½½å·¦è¾¹çš„è„šæœ¬`experiments/tiny/train.py`ï¼Œåœ¨å®éªŒæœåŠ¡å™¨ä¸Šè¿è¡Œã€‚</li>
</ul>
</div>

1. å¯¼å…¥ç›¸å…³åº“

```python
import datasets
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel
from transformers import AutoModelForCausalLM
from transformers import TrainingArguments, Seq2SeqTrainingArguments
from transformers import Trainer, Seq2SeqTrainer
import transformers
from transformers import DataCollatorWithPadding
from transformers import TextGenerationPipeline
import torch
import numpy as np
import os, re
from tqdm import tqdm
import torch.nn as nn
```

2. åŠ è½½**æ•°æ®é›†**

é€šè¿‡HuggingFaceï¼Œå¯ä»¥æŒ‡å®šæ•°æ®é›†åç§°ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½

```python
# æ•°æ®é›†åç§°
DATASET_NAME = "rotten_tomatoes" 

# åŠ è½½æ•°æ®é›†
raw_datasets = load_dataset(DATASET_NAME)

# è®­ç»ƒé›†
raw_train_dataset = raw_datasets["train"]

# éªŒè¯é›†
raw_valid_dataset = raw_datasets["validation"]
```

3. åŠ è½½**æ¨¡å‹**

é€šè¿‡HuggingFaceï¼Œå¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½

```python
# æ¨¡å‹åç§°
MODEL_NAME = "gpt2" 

# åŠ è½½æ¨¡å‹ 
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME,trust_remote_code=True)
```

4. åŠ è½½ **Tokenizer**

é€šè¿‡HuggingFaceï¼Œå¯ä»¥æŒ‡å®šæ¨¡å‹åç§°ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½å¯¹åº”Tokenizer

```python
# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,trust_remote_code=True)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token_id = 0
```

```python
# å…¶å®ƒç›¸å…³å…¬å…±å˜é‡èµ‹å€¼

# è®¾ç½®éšæœºç§å­ï¼šåŒä¸ªç§å­çš„éšæœºåºåˆ—å¯å¤ç°
transformers.set_seed(42)

# æ ‡ç­¾é›†
named_labels = ['neg','pos']

# æ ‡ç­¾è½¬ token_id
label_ids = [
    tokenizer(named_labels[i],add_special_tokens=False)["input_ids"][0] 
    for i in range(len(named_labels))
]
```

5. **å¤„ç†æ•°æ®é›†**ï¼šè½¬æˆæ¨¡å‹æ¥å—çš„è¾“å…¥æ ¼å¼
   - æ‹¼æ¥è¾“å…¥è¾“å‡ºï¼š\<INPUT TOKEN IDS\>\<EOS_TOKEN_ID\>\<OUTPUT TOKEN IDS\>
   - PADæˆç›¸ç­‰é•¿åº¦ï¼š
      - <INPUT 1.1><INPUT 1.2>...\<EOS_TOKEN_ID\>\<OUTPUT TOKEN IDS\>\<PAD\>...\<PAD\>
      - <INPUT 2.1><INPUT 2.2>...\<EOS_TOKEN_ID\>\<OUTPUT TOKEN IDS\>\<PAD\>...\<PAD\>
   - æ ‡è¯†å‡ºå‚ä¸ Loss è®¡ç®—çš„ Tokens (åªæœ‰è¾“å‡º Token å‚ä¸ Loss è®¡ç®—)
      - \<-100\>\<-100\>...\<OUTPUT TOKEN IDS\>\<-100\>...\<-100\>

```python
MAX_LEN=32   #æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¾“å…¥+è¾“å‡ºï¼‰
DATA_BODY_KEY = "text" # æ•°æ®é›†ä¸­çš„è¾“å…¥å­—æ®µå
DATA_LABEL_KEY = "label" #æ•°æ®é›†ä¸­è¾“å‡ºå­—æ®µå

# å®šä¹‰æ•°æ®å¤„ç†å‡½æ•°ï¼ŒæŠŠåŸå§‹æ•°æ®è½¬æˆinput_ids, attention_mask, labels
def process_fn(examples):
    model_inputs = {
            "input_ids": [],
            "attention_mask": [],
            "labels": [],
        }
    for i in range(len(examples[DATA_BODY_KEY])):
        inputs = tokenizer(examples[DATA_BODY_KEY][i],add_special_tokens=False)
        label = label_ids[examples[DATA_LABEL_KEY][i]]
        input_ids = inputs["input_ids"] + [tokenizer.eos_token_id, label]
        
        raw_len = len(input_ids)
        input_len = len(inputs["input_ids"]) + 1

        if raw_len >= MAX_LEN:
            input_ids = input_ids[-MAX_LEN:]
            attention_mask = [1] * MAX_LEN
            labels = [-100]*(MAX_LEN - 1) + [label]
        else:
            input_ids = input_ids + [0] * (MAX_LEN - raw_len)
            attention_mask = [1] * raw_len + [tokenizer.pad_token_id] * (MAX_LEN - raw_len)
            labels = [-100]*input_len + [label] + [-100] * (MAX_LEN - raw_len)
        model_inputs["input_ids"].append(input_ids)
        model_inputs["attention_mask"].append(attention_mask)
        model_inputs["labels"].append(labels)
    return model_inputs
```

```python
# å¤„ç†è®­ç»ƒæ•°æ®é›†
tokenized_train_dataset = raw_train_dataset.map(
    process_fn,
    batched=True,
    remove_columns=raw_train_dataset.columns,
    desc="Running tokenizer on train dataset",
)

# å¤„ç†éªŒè¯æ•°æ®é›†
tokenized_valid_dataset = raw_valid_dataset.map(
    process_fn,
    batched=True,
    remove_columns=raw_valid_dataset.columns,
    desc="Running tokenizer on validation dataset",
)
```

6. å®šä¹‰**æ•°æ®è§„æ•´å™¨**ï¼šè®­ç»ƒæ—¶è‡ªåŠ¨å°†æ•°æ®æ‹†åˆ†æˆ **Batch**

```python
# å®šä¹‰æ•°æ®æ ¡å‡†å™¨ï¼ˆè‡ªåŠ¨ç”Ÿæˆbatchï¼‰
collater = DataCollatorWithPadding(
    tokenizer=tokenizer, return_tensors="pt",
)
```

7. å®šä¹‰è®­ç»ƒ **è¶…å‚**ï¼šæ¯”å¦‚**å­¦ä¹ ç‡**

```python
LR=2e-5         # å­¦ä¹ ç‡
BATCH_SIZE=8    # Batchå¤§å°
INTERVAL=100    # æ¯å¤šå°‘æ­¥æ‰“ä¸€æ¬¡ log / åšä¸€æ¬¡ eval

# å®šä¹‰è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./output",              # checkpointä¿å­˜è·¯å¾„
    evaluation_strategy="steps",        # æ¯Næ­¥åšä¸€æ¬¡eval
    overwrite_output_dir=True,
    num_train_epochs=1,                 # è®­ç»ƒepochæ•°
    per_device_train_batch_size=BATCH_SIZE,     # æ¯å¼ å¡çš„batchå¤§å°
    gradient_accumulation_steps=1,              # ç´¯åŠ å‡ ä¸ªstepåšä¸€æ¬¡å‚æ•°æ›´æ–°
    per_device_eval_batch_size=BATCH_SIZE,      # evaluation batch size
    logging_steps=INTERVAL,             # æ¯20æ­¥evalä¸€æ¬¡
    save_steps=INTERVAL,                # æ¯20æ­¥ä¿å­˜ä¸€ä¸ªcheckpoint
    learning_rate=LR,                   # å­¦ä¹ ç‡
)
```

8. å®šä¹‰**è®­ç»ƒå™¨**

```python
# èŠ‚çœæ˜¾å­˜
model.gradient_checkpointing_enable()

# å®šä¹‰è®­ç»ƒå™¨
trainer = Trainer(
    model=model, # å¾…è®­ç»ƒæ¨¡å‹
    args=training_args, # è®­ç»ƒå‚æ•°
    data_collator=collater, # æ•°æ®æ ¡å‡†å™¨
    train_dataset=tokenized_train_dataset,  # è®­ç»ƒé›†
    eval_dataset=tokenized_valid_dataset,   # éªŒè¯é›†
    # compute_metrics=compute_metric,         # è®¡ç®—è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
)
```

9. å¼€å§‹è®­ç»ƒ

```python
# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### æ€»ç»“ä¸Šè¿°è¿‡ç¨‹

1. åŠ è½½æ•°æ®é›†
2. æ•°æ®é¢„å¤„ç†ï¼š
   - å°†è¾“å…¥è¾“å‡ºæŒ‰ç‰¹å®šæ ¼å¼æ‹¼æ¥
   - æ–‡æœ¬è½¬ Token IDs
   - é€šè¿‡ labels æ ‡è¯†å‡ºå“ªéƒ¨åˆ†æ˜¯è¾“å‡ºï¼ˆåªæœ‰è¾“å‡ºçš„ token å‚ä¸ loss è®¡ç®—ï¼‰
4. åŠ è½½æ¨¡å‹ã€Tokenizer
5. å®šä¹‰æ•°æ®è§„æ•´å™¨
6. å®šä¹‰è®­ç»ƒè¶…å‚ï¼šå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€...
7. å®šä¹‰è®­ç»ƒå™¨
8. å¼€å§‹è®­ç»ƒ

<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b> 
    <ul>
        <li>è®°ä½ä¸Šé¢çš„æµç¨‹ï¼Œä½ å°±èƒ½è·‘é€šæ¨¡å‹è®­ç»ƒè¿‡ç¨‹</li>
        <li>ç†è§£ä¸‹é¢çš„çŸ¥è¯†ï¼Œä½ å°±èƒ½è®­ç»ƒå¥½æ¨¡å‹æ•ˆæœ</li>
    </ul>
</div>

## äºŒã€ä»€ä¹ˆæ˜¯æ¨¡å‹

<div class="alert alert-warning">
<b>å°è¯•ï¼š</b> ç”¨ç®€å•çš„æ•°å­¦è¯­è¨€è¡¨è¾¾æ¦‚å¿µ
</div>

### 2.1ã€é€šä¿—ï¼ˆä¸ä¸¥è°¨ï¼‰çš„è¯´ã€**æ¨¡å‹**æ˜¯ä¸€ä¸ªå‡½æ•°ï¼š

$y=F(x;\omega)$

- å®ƒæ¥æ”¶è¾“å…¥$x$ï¼šå¯ä»¥æ˜¯ä¸€ä¸ªè¯ã€ä¸€ä¸ªå¥å­ã€ä¸€ç¯‡æ–‡ç« æˆ–å›¾ç‰‡ã€è¯­éŸ³ã€è§†é¢‘ ...
  - è¿™äº›ç‰©ä½“éƒ½è¢«è¡¨ç¤ºæˆä¸€ä¸ªæ•°å­¦ã€ŒçŸ©é˜µã€ï¼ˆå…¶å®åº”è¯¥å«å¼ é‡ï¼Œtensorï¼‰
- å®ƒé¢„æµ‹è¾“å‡º$y$
  - å¯ä»¥æ˜¯ã€Œæ˜¯å¦ã€ï¼ˆ{0,1}ï¼‰ã€æ ‡ç­¾ï¼ˆ{0,1,2,3...}ï¼‰ã€ä¸€ä¸ªæ•°å€¼ï¼ˆå›å½’é—®é¢˜ï¼‰ã€ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ ...
- å®ƒçš„è¡¨è¾¾å¼å°±æ˜¯ç½‘ç»œç»“æ„ï¼ˆè¿™é‡Œç‰¹æŒ‡æ·±åº¦å­¦ä¹ ï¼‰
- å®ƒæœ‰ä¸€ç»„**å‚æ•°** $\omega$ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è¦è®­ç»ƒçš„éƒ¨åˆ†

<div class="alert alert-warning">
<b>æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªæ–¹ç¨‹ï¼š</b> 
    <ol>
        <li>æ¯æ¡æ•°æ®å°±æ˜¯ä¸€å¯¹å„¿ $(x,y)$ ï¼Œå®ƒä»¬æ˜¯å¸¸é‡</li>
        <li>å‚æ•°æ˜¯æœªçŸ¥æ•°ï¼Œæ˜¯å˜é‡</li>
        <li>$F$ å°±æ˜¯è¡¨è¾¾å¼ï¼šæˆ‘ä»¬ä¸çŸ¥é“çœŸå®çš„å…¬å¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œæ‰€ä»¥å‡è®¾äº†ä¸€ä¸ªè¶³å¤Ÿå¤æ‚çš„å…¬å¼ï¼ˆæ¯”å¦‚ï¼Œä¸€ä¸ªç‰¹å®šç»“æ„çš„ç¥ç»ç½‘ç»œï¼‰</li>
        <li>è¿™ä¸ªæ±‚è§£è¿™ä¸ªæ–¹ç¨‹ï¼ˆè¿‘ä¼¼è§£ï¼‰å°±æ˜¯è®­ç»ƒè¿‡ç¨‹</li>
    </ol>
</div>

<div class="alert alert-success">
<b>é€šä¿—çš„è®²ï¼š</b> è®­ç»ƒï¼Œå°±æ˜¯ç¡®å®šè¿™ç»„å‚æ•°çš„å–å€¼
    <ul>
        <li>ç”¨æ•°å­¦ï¼ˆæ•°å€¼åˆ†æï¼‰æ–¹æ³•æ‰¾åˆ°ä½¿æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°è¶³å¤Ÿå¥½çš„ä¸€ä¸ªå€¼</li>
        <li>è¡¨ç°è¶³å¤Ÿå¥½ï¼Œå°±æ˜¯è¯´ï¼Œå¯¹æ¯ä¸ªæ•°æ®æ ·æœ¬$(x,y)$ï¼Œä½¿ $F(x;\omega)$ çš„å€¼å°½å¯èƒ½æ¥è¿‘ $y$</li>
    </ul>
</div>

### 2.2ã€ä¸€ä¸ªæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ

ä¸€ä¸ªç¥ç»å…ƒï¼š$y=f(\sum_i w_i\cdot x_i)$

<img src="_images/llm/neuron.jpg" style="margin-left: 0px" width="600px">

æŠŠå¾ˆå¤šç¥ç»å…ƒè¿æ¥èµ·æ¥ï¼Œå°±æˆäº†ç¥ç»ç½‘ç»œï¼š$y=f(\sum_i w_i\cdot x_i)$ã€$z=f(\sum_i w'_i\cdot y_i)$ã€$\tau=f(\sum_i w''_i\cdot z_i)$ã€...

<img src="_images/llm/network.jpg" style="margin-left: 0px" width="600px">

è¿™é‡Œçš„$f$å«æ¿€æ´»å‡½æ•°ï¼Œæœ‰å¾ˆå¤šç§å½¢å¼

ç°ä»Šçš„å¤§æ¨¡å‹ä¸­å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ï¼šReLUã€GELUã€Swish

<img src="_images/llm/activation.jpeg" style="margin-left: 0px" width="600px">

<div class="alert alert-warning">
<b>æ€è€ƒï¼š</b> è¿™é‡Œå¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ä¼šæ€æ ·ï¼Ÿ
</div>


## ä¸‰ã€ä»€ä¹ˆæ˜¯æ¨¡å‹è®­ç»ƒ

æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ç»„å‚æ•°$\omega$ï¼Œä½¿æ¨¡å‹é¢„æµ‹çš„è¾“å‡º$\hat{y}=F(x;\omega)$ä¸çœŸå®çš„è¾“å‡º$y$ï¼Œå°½å¯èƒ½çš„æ¥è¿‘

è¿™é‡Œï¼Œæˆ‘ä»¬ï¼ˆè‡³å°‘ï¼‰éœ€è¦ä¸¤ä¸ªè¦ç´ ï¼š

- ä¸€ä¸ªæ•°æ®é›†ï¼ŒåŒ…å«$N$ä¸ªè¾“å…¥è¾“å‡ºçš„ä¾‹å­ï¼ˆç§°ä¸ºæ ·æœ¬ï¼‰ï¼š$D=\{(x_i,y_i)\}_{i=1}^N$
- ä¸€ä¸ª**æŸå¤±å‡½æ•°**ï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹çš„è¾“å‡ºä¸çœŸå®è¾“å‡ºä¹‹é—´çš„å·®è·ï¼š$\mathrm{loss}(y,F(x;\omega))$

### 3.1ã€æ¨¡å‹è®­ç»ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ±‚è§£æœ€ä¼˜åŒ–é—®é¢˜çš„è¿‡ç¨‹

$\min_{\omega} L(D,\omega)$

$L(D,\omega)=\frac{1}{N}\sum_{i=1}^N\mathrm{loss}(y,F(x;\omega))$

### 3.2ã€æ€ä¹ˆæ±‚è§£

å›å¿†ä¸€ä¸‹æ¢¯åº¦çš„å®šä¹‰

<img src="_images/llm/gradient.svg" style="margin-left: 0px" width="400px">

ä»æœ€ç®€å•çš„æƒ…å†µè¯´èµ·ï¼šæ¢¯åº¦ä¸‹é™ä¸å‡¸é—®é¢˜

<img src="_images/llm/gradient.png" style="margin-left: 0px" width="600px">

æ¢¯åº¦å†³å®šäº†å‡½æ•°å˜åŒ–çš„æ–¹å‘ï¼Œæ¯æ¬¡è¿­ä»£æ›´æ–°æˆ‘ä»¬ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªæå€¼

$\omega_{n+1}\leftarrow \omega_n - \gamma \nabla_{\omega}L(D,\omega)$

å…¶ä¸­ï¼Œ$\gamma<1$å«åš**å­¦ä¹ ç‡**ï¼Œå®ƒå’Œæ¢¯åº¦çš„æ¨¡æ•°å…±åŒå†³å®šäº†æ¯æ­¥èµ°å¤šè¿œ

### 3.3ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ1ï¼‰**ï¼šåœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šæ±‚æ¢¯åº¦ï¼Œè®¡ç®—é‡å¤ªå¤§äº†

<br/>
<img src="_images/llm/11batch.png" style="margin-left: 0px" width="600px">

<div class="alert alert-success">
<b>ç»éªŒï¼š</b>
    <ul>
        <li>å¦‚æœå…¨é‡å‚æ•°è®­ç»ƒï¼šæ¡ä»¶å…è®¸çš„æƒ…å†µä¸‹ï¼Œå…ˆå°è¯•Batch Sizeå¤§äº›</li>
        <li>å°å‚æ•°é‡å¾®è°ƒï¼šBatch Size å¤§ä¸ä¸€å®šå°±å¥½ï¼Œçœ‹ç¨³å®šæ€§</li>
    </ul>
</div>

### 3.4ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ2ï¼‰**ï¼šæ·±åº¦å­¦ä¹ æ²¡æœ‰å…¨å±€æœ€ä¼˜è§£ï¼ˆéå‡¸é—®é¢˜ï¼‰

<br/>
<img src="_images/llm/local_minima.png" style="margin-left: 0px" width="600px">

### 3.5ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ3ï¼‰**ï¼šå­¦ä¹ ç‡ä¹Ÿå¾ˆå…³é”®ï¼Œç”šè‡³éœ€è¦åŠ¨æ€è°ƒæ•´

<br/>
<img src="_images/llm/lr.png" style="margin-left: 0px" width="600px">

<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>é€‚å½“è°ƒæ•´å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰ï¼Œé¿å…é™·å…¥å¾ˆå·®çš„å±€éƒ¨è§£æˆ–è€…è·³è¿‡äº†å¥½çš„è§£
</div>

## å››ã€æ±‚è§£å™¨

ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹æ›´å¥½çš„æ”¶æ•›ï¼Œäººä»¬è®¾è®¡äº†å¾ˆå¤šæ›´å¤æ‚çš„æ±‚è§£å™¨

- æ¯”å¦‚ï¼šSGDã€L-BFGSã€Rpropã€RMSpropã€Adamã€AdamWã€AdaGradã€AdaDelta ç­‰ç­‰
- ä½†æ˜¯ï¼Œå¥½åœ¨å¯¹äºTransformeræœ€å¸¸ç”¨çš„å°±æ˜¯ Adam æˆ–è€… AdamW

## äº”ã€ä¸€äº›å¸¸ç”¨çš„**æŸå¤±å‡½æ•°**

- ä¸¤ä¸ªæ•°å€¼çš„å·®è·ï¼ŒMean Square Errorï¼š$\ell_{\mathrm{MSE}}=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y}_i)^2$ (ç­‰ä»·äºæ¬§å¼è·ç¦»ï¼Œè§ä¸‹æ–‡)
- ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ï¼ˆæ¬§å¼ï¼‰è·ç¦»ï¼š$\ell(\mathbf{y},\mathbf{\hat{y}})=\|\mathbf{y}-\mathbf{\hat{y}}\|$
- ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’ï¼ˆä½™å¼¦è·ç¦»ï¼‰ï¼š
  <img src="_images/llm/cosine_loss.png" style="margin-left: 0px" width="400px">

- ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œäº¤å‰ç†µï¼š$\ell_{\mathrm{CE}}(p,q)=-\sum_i p_i\log q_i$ â€”â€”å‡è®¾æ˜¯æ¦‚ç‡åˆ†å¸ƒ p,q æ˜¯ç¦»æ•£çš„
- è¿™äº›æŸå¤±å‡½æ•°ä¹Ÿå¯ä»¥ç»„åˆä½¿ç”¨ï¼ˆåœ¨æ¨¡å‹è’¸é¦çš„åœºæ™¯å¸¸è§è¿™ç§æƒ…å†µï¼‰ï¼Œä¾‹å¦‚$L=L_1+\lambda L_2$ï¼Œå…¶ä¸­$\lambda$æ˜¯ä¸€ä¸ªé¢„å…ˆå®šä¹‰çš„æƒé‡ï¼Œä¹Ÿå«ä¸€ä¸ªã€Œè¶…å‚ã€

<div class="alert alert-warning">
<b>æ€è€ƒï¼š</b> ä½ èƒ½æ‰¾åˆ°è¿™äº›æŸå¤±å‡½æ•°å’Œåˆ†ç±»ã€èšç±»ã€å›å½’é—®é¢˜ä¹‹é—´çš„å…³ç³»å—ï¼Ÿ
</div>


## å…­ã€å†åŠ¨æ‰‹å¤ä¹ ä¸€ä¸‹ä¸Šè¿°è¿‡ç¨‹

ç”¨ PyTorch è®­ç»ƒä¸€ä¸ªæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ

æ•°æ®é›†ï¼ˆMNISTï¼‰æ ·ä¾‹ï¼š

<img src="_images/llm/MNIST.jpg" style="margin-left: 0px" width="600px">

è¾“å…¥ä¸€å¼  28Ã—28 çš„å›¾åƒï¼Œè¾“å‡ºæ ‡ç­¾ 0--9


```python
from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR

BATCH_SIZE = 64
TEST_BACTH_SIZE = 1000
EPOCHS = 15
LR = 0.01
SEED = 42
LOG_INTERVAL = 100

# å®šä¹‰ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œ
class FeedForwardNet(nn.Module):
    def __init__(self):
        super().__init__()
        # ç¬¬ä¸€å±‚784ç»´è¾“å…¥ã€256ç»´è¾“å‡º -- å›¾åƒå¤§å°28Ã—28=784
        self.fc1 = nn.Linear(784, 256)
        # ç¬¬äºŒå±‚256ç»´è¾“å…¥ã€128ç»´è¾“å‡º
        self.fc2 = nn.Linear(256, 128)
        # ç¬¬ä¸‰å±‚128ç»´è¾“å…¥ã€64ç»´è¾“å‡º
        self.fc3 = nn.Linear(128, 64)
        # ç¬¬å››å±‚64ç»´è¾“å…¥ã€10ç»´è¾“å‡º -- è¾“å‡ºç±»åˆ«10ç±»ï¼ˆ0,1,...9ï¼‰
        self.fc4 = nn.Linear(64, 10)

        # Dropout module with 0.2 drop probability
        self.dropout = nn.Dropout(p=0.2)

    def forward(self, x):
        # æŠŠè¾“å…¥å±•å¹³æˆ1Då‘é‡
        x = x.view(x.shape[0], -1)

        # æ¯å±‚æ¿€æ´»å‡½æ•°æ˜¯ReLUï¼Œé¢å¤–åŠ dropout
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.dropout(F.relu(self.fc2(x)))
        x = self.dropout(F.relu(self.fc3(x)))

        # è¾“å‡ºä¸º10ç»´æ¦‚ç‡åˆ†å¸ƒ
        x = F.log_softmax(self.fc4(x), dim=1)

        return x

# è®­ç»ƒè¿‡ç¨‹
def train(model, loss_fn, device, train_loader, optimizer, epoch):
    # å¼€å¯æ¢¯åº¦è®¡ç®—
    model.train()
    for batch_idx, (data_input, true_label) in enumerate(train_loader):
        # ä»æ•°æ®åŠ è½½å™¨è¯»å–ä¸€ä¸ªbatch
        # æŠŠæ•°æ®ä¸Šè½½åˆ°GPUï¼ˆå¦‚æœ‰ï¼‰
        data_input, true_label = data_input.to(device), true_label.to(device)
        # æ±‚è§£å™¨åˆå§‹åŒ–ï¼ˆæ¯ä¸ªbatchåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        optimizer.zero_grad()
        # æ­£å‘ä¼ æ’­ï¼šæ¨¡å‹ç”±è¾“å…¥é¢„æµ‹è¾“å‡º
        output = model(data_input)
        # è®¡ç®—loss
        loss = loss_fn(output, true_label) 
        # åå‘ä¼ æ’­ï¼šè®¡ç®—å½“å‰batchçš„lossçš„æ¢¯åº¦
        loss.backward()
        # ç”±æ±‚è§£å™¨æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°
        optimizer.step()

        # é—´éš”æ€§çš„è¾“å‡ºå½“å‰batchçš„è®­ç»ƒloss
        if batch_idx % LOG_INTERVAL == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data_input), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))


# è®¡ç®—åœ¨æµ‹è¯•é›†çš„å‡†ç¡®ç‡å’Œloss
def test(model, loss_fn, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            # sum up batch loss
            test_loss += loss_fn(output, target, reduction='sum').item()
            # get the index of the max log-probability
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


def main():
    # æ£€æŸ¥æ˜¯å¦æœ‰GPU
    use_cuda = torch.cuda.is_available()

    # è®¾ç½®éšæœºç§å­ï¼ˆä»¥ä¿è¯ç»“æœå¯å¤ç°ï¼‰
    torch.manual_seed(SEED)

    # è®­ç»ƒè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
    device = torch.device("cuda" if use_cuda else "cpu")

    # è®¾ç½®batch size
    train_kwargs = {'batch_size': BATCH_SIZE}
    test_kwargs = {'batch_size': TEST_BACTH_SIZE}

    if use_cuda:
        cuda_kwargs = {'num_workers': 1,
                       'pin_memory': True,
                       'shuffle': True}
        train_kwargs.update(cuda_kwargs)
        test_kwargs.update(cuda_kwargs)

    # æ•°æ®é¢„å¤„ç†ï¼ˆè½¬tensorã€æ•°å€¼å½’ä¸€åŒ–ï¼‰
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    # è‡ªåŠ¨ä¸‹è½½MNISTæ•°æ®é›†
    dataset_train = datasets.MNIST('data', train=True, download=True,
                                   transform=transform)
    dataset_test = datasets.MNIST('data', train=False,
                                  transform=transform)

    # å®šä¹‰æ•°æ®åŠ è½½å™¨ï¼ˆè‡ªåŠ¨å¯¹æ•°æ®åŠ è½½ã€å¤šçº¿ç¨‹ã€éšæœºåŒ–ã€åˆ’åˆ†batchã€ç­‰ç­‰ï¼‰
    train_loader = torch.utils.data.DataLoader(dataset_train, **train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset_test, **test_kwargs)

    # åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹
    model = FeedForwardNet().to(device)

    # æŒ‡å®šæ±‚è§£å™¨
    optimizer = optim.SGD(model.parameters(), lr=LR)
    # scheduler = StepLR(optimizer, step_size=1, gamma=0.9)

    # å®šä¹‰losså‡½æ•°
    # æ³¨ï¼šnll ä½œç”¨äº log_softmax ç­‰ä»·äºäº¤å‰ç†µï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥è‡ªè¡Œæ¨å¯¼
    # https://blog.csdn.net/weixin_38145317/article/details/103288032
    loss_fn = F.nll_loss

    # è®­ç»ƒNä¸ªepoch
    for epoch in range(1, EPOCHS + 1):
        train(model, loss_fn, device, train_loader, optimizer, epoch)
        test(model, loss_fn, device, test_loader)
        # scheduler.step()


if __name__ == '__main__':
    main()
```

<div class="alert alert-warning">
<b>å¦‚ä½•è¿è¡Œè¿™æ®µä»£ç ï¼š</b>
<ol>
    <li>ä¸è¦åœ¨Jupyterç¬”è®°ä¸Šç›´æ¥è¿è¡Œ</li>
    <li>è¯·å°†å·¦ä¾§çš„ `experiments/mnist/train.py` æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°</li>
    <li>å®‰è£…ç›¸å…³ä¾èµ–åŒ…: pip install torch torchvision</li>
    <li>è¿è¡Œï¼špython3 train.py</li>
</ol>
</div>


## [é€‰ä¿®å†…å®¹](optional/index.ipynb)

æ·±åº¦å­¦ä¹ ä¸­è¿˜æœ‰å¾ˆå¤šå¸¸ç”¨æŠ€å·§ï¼Œæ¶‰åŠçš„èƒŒæ™¯çŸ¥è¯†è¾ƒæ·±ï¼Œä¸åœ¨è¯¾ä¸Šå±•å¼€äº†ã€‚

æˆ‘ä»¬é€‰äº†ä¸€äº›æœ€å¸¸ç”¨çš„ï¼Œæ•´ç†æˆäº†éƒ¨åˆ†é€‰ä¿®å†…å®¹ã€‚

å»ºè®®æœ‰æ·±å…¥å­¦ä¹ éœ€æ±‚çš„åŒå­¦è‡ªè¡Œé˜…è¯»ï¼Œæœ‰ç–‘é—®çš„å¯ä»¥åœ¨ç­”ç–‘è¯¾æå‡ºã€‚

## ä½œä¸š

åœ¨ HuggingFace ä¸Šæ‰¾ä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼Œè‡ªå·±å®ç°ä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹
