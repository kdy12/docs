## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ 

1. å¦‚ä½•ä½¿ç”¨ LangChainï¼šä¸€å¥—åœ¨å¤§æ¨¡å‹èƒ½åŠ›ä¸Šå°è£…çš„å·¥å…·æ¡†æ¶
2. å¦‚ä½•ç”¨å‡ è¡Œä»£ç å®ç°ä¸€ä¸ªå¤æ‚çš„ AI åº”ç”¨
3. é¢å‘å¤§æ¨¡å‹çš„æµç¨‹å¼€å‘çš„è¿‡ç¨‹æŠ½è±¡

å¼€å§‹ä¸Šè¯¾ï¼


## å†™åœ¨å‰é¢

- LangChain ä¹Ÿæ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰
- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹
- LangChain è¿­ä»£é€Ÿåº¦æ˜æ˜¾å¿«äº Semantic Kernelï¼Œå‡ ä¹æ˜å¤©ä¸€ä¸ªç‰ˆæœ¬
- å­¦ä¹  Langchain è¦å…³æ³¨æ¥å£å˜æ›´


## LangChain vs. Semantic Kernel

[![Star History Chart](https://api.star-history.com/svg?repos=langchain-ai/langchain,microsoft/semantic-kernel,langchain-ai/langchainjs&type=Date)](https://star-history.com/#langchain-ai/langchain&microsoft/semantic-kernel&langchain-ai/langchainjs&Date)

æ•°æ®æ¥æºï¼šhttps://star-history.com/#langchain-ai/langchain&microsoft/semantic-kernel&langchain-ai/langchainjs&Date

## LangChain çš„æ ¸å¿ƒç»„ä»¶

1. æ¨¡å‹ I/O å°è£…
   - LLMsï¼šå¤§è¯­è¨€æ¨¡å‹
   - Chat Modelsï¼šä¸€èˆ¬åŸºäº LLMsï¼Œä½†æŒ‰å¯¹è¯ç»“æ„é‡æ–°å°è£…
   - PromptTempleï¼šæç¤ºè¯æ¨¡æ¿
   - OutputParserï¼šè§£æè¾“å‡º
2. æ•°æ®è¿æ¥å°è£…
   - Document Loadersï¼šå„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨
   - Document Transformersï¼šå¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc
   - Text Embedding Modelsï¼šæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œï¼ˆå•¥æ„æ€ï¼Ÿåˆ«æ€¥ï¼Œåé¢è¯¦ç»†è®²ï¼‰
   - Verctorstores: ï¼ˆé¢å‘æ£€ç´¢çš„ï¼‰å‘é‡çš„å­˜å‚¨
   - Retrievers: å‘é‡çš„æ£€ç´¢
3. è®°å¿†å°è£…
   - Memoryï¼šè¿™é‡Œä¸æ˜¯ç‰©ç†å†…å­˜ï¼Œä»æ–‡æœ¬çš„è§’åº¦ï¼Œå¯ä»¥ç†è§£ä¸ºâ€œä¸Šæ–‡â€ã€â€œå†å²è®°å½•â€æˆ–è€…è¯´â€œè®°å¿†åŠ›â€çš„ç®¡ç†
4. æ¶æ„å°è£…
   - Chainï¼šå®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ
   - Agentï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·ï¼Œæœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½
     - Toolsï¼šè°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰
     - Toolkitsï¼šæ“ä½œæŸè½¯ä»¶çš„ä¸€ç»„å·¥å…·é›†ï¼Œä¾‹å¦‚ï¼šæ“ä½œ DBã€æ“ä½œ Gmail ç­‰ç­‰
5. Callbacks

<img src="_images/llm/langchain.png" style="margin-left: 0px" width=500px>

å®˜æ–¹æ–‡æ¡£åœ°å€ï¼šhttps://python.langchain.com/docs/get_started

## ä¸€ã€æ¨¡å‹ I/O å°è£…

æŠŠä¸åŒçš„æ¨¡å‹ï¼Œç»Ÿä¸€å°è£…æˆä¸€ä¸ªæ¥å£ï¼Œæ–¹ä¾¿æ›´æ¢æ¨¡å‹è€Œä¸ç”¨é‡æ„ä»£ç ã€‚

### 1.1 æ¨¡å‹ APIï¼šLLM vs. ChatModel



```python
#å®‰è£…æœ€æ–°ç‰ˆæœ¬
!pip install langchain==0.1.0
!pip install langchain-openai # v0.1.0æ–°å¢çš„åº•åŒ…
```

### 1.1.1 OpenAI æ¨¡å‹å°è£…


```python
from langchain_openai import ChatOpenAI
 
llm = ChatOpenAI(model="gpt-4") # é»˜è®¤æ˜¯gpt-3.5-turbo
response = llm.invoke("ä½ æ˜¯è°")
print(response.content)
```

    æˆ‘æ˜¯OpenAIçš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘è¢«è®¾è®¡å‡ºæ¥æ˜¯ä¸ºäº†å¸®åŠ©è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œå¸®åŠ©ç”¨æˆ·å®Œæˆå„ç§ä»»åŠ¡ã€‚


### 1.1.2 å¤šè½®å¯¹è¯ Session å°è£…


```python
from langchain.schema import (
    AIMessage, #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role
    HumanMessage, #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role
    SystemMessage #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role
)

messages = [
    SystemMessage(content="ä½ æ˜¯AGIClassçš„è¯¾ç¨‹åŠ©ç†ã€‚"), 
    HumanMessage(content="æˆ‘æ˜¯å­¦å‘˜ï¼Œæˆ‘å«ç‹å“ç„¶ã€‚"), 
    AIMessage(content="æ¬¢è¿ï¼"),
    HumanMessage(content="æˆ‘æ˜¯è°") 
]
llm.invoke(messages) 
```




    AIMessage(content='æ‚¨æ˜¯å­¦å‘˜ç‹å“ç„¶ã€‚')



<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>é€šè¿‡æ¨¡å‹å°è£…ï¼Œå®ç°ä¸åŒæ¨¡å‹çš„ç»Ÿä¸€æ¥å£è°ƒç”¨
</div>


```python
# å…¶å®ƒæ¨¡å‹åˆ†è£…åœ¨ langchain_community åº•åŒ…ä¸­
from langchain_community.chat_models import ErnieBotChat
from langchain.schema import HumanMessage

ernie = ErnieBotChat()

messages = [
    HumanMessage(content="ä½ æ˜¯è°") 
]

ernie.invoke(messages)
```




    AIMessage(content='æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç™¾åº¦ç ”å‘çš„çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼Œä¸­æ–‡åæ˜¯æ–‡å¿ƒä¸€è¨€ï¼Œè‹±æ–‡åæ˜¯ERNIE Botã€‚æˆ‘èƒ½å¤Ÿä¸äººå¯¹è¯äº’åŠ¨ï¼Œå›ç­”é—®é¢˜ï¼ŒååŠ©åˆ›ä½œï¼Œé«˜æ•ˆä¾¿æ·åœ°å¸®åŠ©äººä»¬è·å–ä¿¡æ¯ã€çŸ¥è¯†å’Œçµæ„Ÿã€‚\n\nå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚')



### 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º

<img src="_images/llm/model_io.jpg" style="margin-left: 0px" width=500px>

### 1.2.1 Promptæ¨¡æ¿å°è£…

PromptTemplate å¯ä»¥åœ¨æ¨¡æ¿ä¸­è‡ªå®šä¹‰å˜é‡


```python
from langchain.prompts import PromptTemplate

template = PromptTemplate.from_template("ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯")
print(template)
print(template.format(subject='å°æ˜'))
```

    input_variables=['subject'] template='ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯'
    ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯


ChatPromptTemplate


```python
from langchain.prompts import ChatPromptTemplate
from langchain.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chat_models import ChatOpenAI

template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}"),
        HumanMessagePromptTemplate.from_template("{query}"),
    ]
)

llm = ChatOpenAI()
prompt = template.format_messages(
        product="AGIè¯¾å ‚",
        name="ç“œç“œ",
        query="ä½ æ˜¯è°"
    )

llm.invoke(prompt)
```




    AIMessage(content='æˆ‘æ˜¯AGIè¯¾å ‚çš„å®¢æœåŠ©æ‰‹ï¼Œåå­—å«ç“œç“œã€‚æˆ‘å¯ä»¥å›ç­”å…³äºAGIè¯¾å ‚çš„é—®é¢˜ï¼Œæä¾›å¸®åŠ©å’Œæ”¯æŒã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ')



<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>æŠŠPromptæ¨¡æ¿çœ‹ä½œå¸¦æœ‰å‚æ•°çš„å‡½æ•°ï¼Œå¯ç±»æ¯”äº SK çš„ Semantic Function
</div>


### 1.2.2ã€ä»æ–‡ä»¶åŠ è½½Promptæ¨¡æ¿

Yamlæ ¼å¼
 _type: prompt
input_variables:
    ["adjective", "content"]
template: 
    Tell me a {adjective} joke about {content}.
JSONæ ¼å¼
{
    "_type": "prompt",
    "input_variables": ["adjective", "content"],
    "template": "Tell me a {adjective} joke about {content}."
}
Templateå•ç‹¬å­˜æ”¾

```sh
cat simple_template.txt
```

```
Tell me a {adjective} joke about {content}.
```
{
    "_type": "prompt",
    "input_variables": ["adjective", "content"],
    "template_path": "simple_template.txt"
}
åŠ è½½æ–¹å¼


```python
from langchain.prompts import load_prompt

prompt = load_prompt("simple_prompt.yaml")

# OR 
# prompt = load_prompt("simple_prompt.json")

print(prompt.format(adjective="funny", content="Xiao Ming"))
```

    Tell me a funny joke about Xiao Ming.


### 1.3 è¾“å‡ºå°è£… OutputParser

è‡ªåŠ¨æŠŠ LLM è¾“å‡ºçš„å­—ç¬¦ä¸²æŒ‰æŒ‡å®šæ ¼å¼åŠ è½½ã€‚

LangChain å†…ç½®çš„ OutputParser åŒ…æ‹¬:

- ListParser
- DatetimeParser
- EnumParser
- PydanticParser
- XMLParser

ç­‰ç­‰

### 1.3.1 Pydantic (JSON) Parser

è‡ªåŠ¨æ ¹æ®Pydanticç±»çš„å®šä¹‰ï¼Œç”Ÿæˆè¾“å‡ºçš„æ ¼å¼è¯´æ˜


```python
from langchain_core.pydantic_v1  import BaseModel, Field, validator
from typing import List, Dict

# å®šä¹‰ä½ çš„è¾“å‡ºå¯¹è±¡
class Date(BaseModel):
    year: int = Field(description="Year")
    month: int = Field(description="Month")
    day: int = Field(description="Day")
    era: str = Field(description="BC or AD")

    # ----- å¯é€‰æœºåˆ¶ --------
    # ä½ å¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„æ ¡éªŒæœºåˆ¶
    @validator('month')
    def valid_month(cls, field):
        if field <= 0 or field > 12:
            raise ValueError("æœˆä»½å¿…é¡»åœ¨1-12ä¹‹é—´")
        return field
        
    @validator('day')
    def valid_day(cls, field):
        if field <= 0 or field > 31:
            raise ValueError("æ—¥æœŸå¿…é¡»åœ¨1-31æ—¥ä¹‹é—´")
        return field

    @validator('day', pre=True, always=True)
    def valid_date(cls, day, values):
        year = values.get('year')
        month = values.get('month')

        # ç¡®ä¿å¹´ä»½å’Œæœˆä»½éƒ½å·²ç»æä¾›
        if year is None or month is None:
            return day  # æ— æ³•éªŒè¯æ—¥æœŸï¼Œå› ä¸ºæ²¡æœ‰å¹´ä»½å’Œæœˆä»½

        # æ£€æŸ¥æ—¥æœŸæ˜¯å¦æœ‰æ•ˆ
        if month == 2:
            if cls.is_leap_year(year) and day > 29:
                raise ValueError("é—°å¹´2æœˆæœ€å¤šæœ‰29å¤©")
            elif not cls.is_leap_year(year) and day > 28:
                raise ValueError("éé—°å¹´2æœˆæœ€å¤šæœ‰28å¤©")
        elif month in [4, 6, 9, 11] and day > 30:
            raise ValueError(f"{month}æœˆæœ€å¤šæœ‰30å¤©")

        return day

    @staticmethod
    def is_leap_year(year):
        if year % 400 == 0 or (year % 4 == 0 and year % 100 != 0):
            return True
        return False
```


```python
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_openai import ChatOpenAI

from langchain.output_parsers import PydanticOutputParser


model_name = 'gpt-4'
temperature = 0
model = ChatOpenAI(model_name=model_name, temperature=temperature)

# æ ¹æ®Pydanticå¯¹è±¡çš„å®šä¹‰ï¼Œæ„é€ ä¸€ä¸ªOutputParser
parser = PydanticOutputParser(pydantic_object=Date)

template = """æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚
{format_instructions}
ç”¨æˆ·è¾“å…¥:
{query}"""

prompt = PromptTemplate(
    template=template,
    input_variables=["query"],
    # ç›´æ¥ä»OutputParserä¸­è·å–è¾“å‡ºæè¿°ï¼Œå¹¶å¯¹æ¨¡æ¿çš„å˜é‡é¢„å…ˆèµ‹å€¼
    partial_variables={"format_instructions": parser.get_format_instructions()} 
)

print("====Format Instruction=====")
print(parser.get_format_instructions())


query = "2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´..."
model_input = prompt.format_prompt(query=query)

print("====Prompt=====")
print(model_input.to_string())

output = model(model_input.to_messages())
print("====æ¨¡å‹åŸå§‹è¾“å‡º=====")
print(output)
print("====Parseåçš„è¾“å‡º=====")
date = parser.parse(output.content)
print(date)
```

    ====Format Instruction=====
    The output should be formatted as a JSON instance that conforms to the JSON schema below.
    
    As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
    the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.
    
    Here is the output schema:
    ```
    {"properties": {"year": {"title": "Year", "description": "Year", "type": "integer"}, "month": {"title": "Month", "description": "Month", "type": "integer"}, "day": {"title": "Day", "description": "Day", "type": "integer"}, "era": {"title": "Era", "description": "BC or AD", "type": "string"}}, "required": ["year", "month", "day", "era"]}
    ```
    ====Prompt=====
    æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚
    The output should be formatted as a JSON instance that conforms to the JSON schema below.
    
    As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
    the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.
    
    Here is the output schema:
    ```
    {"properties": {"year": {"title": "Year", "description": "Year", "type": "integer"}, "month": {"title": "Month", "description": "Month", "type": "integer"}, "day": {"title": "Day", "description": "Day", "type": "integer"}, "era": {"title": "Era", "description": "BC or AD", "type": "string"}}, "required": ["year", "month", "day", "era"]}
    ```
    ç”¨æˆ·è¾“å…¥:
    2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...
    ====æ¨¡å‹åŸå§‹è¾“å‡º=====
    content='{"year": 2023, "month": 4, "day": 6, "era": "AD"}'
    ====Parseåçš„è¾“å‡º=====
    year=2023 month=4 day=6 era='AD'


### 1.3.2 Auto-Fixing Parser

åˆ©ç”¨LLMè‡ªåŠ¨æ ¹æ®è§£æå¼‚å¸¸ä¿®å¤å¹¶é‡æ–°è§£æ


```python
from langchain.output_parsers import OutputFixingParser

new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI(model="gpt-4"))

#æˆ‘ä»¬æŠŠä¹‹å‰outputçš„æ ¼å¼æ”¹é”™
output = output.content.replace("4","å››æœˆ")
print("===æ ¼å¼é”™è¯¯çš„Output===")
print(output)
try:
    date = parser.parse(output)
except Exception as e:
    print("===å‡ºç°å¼‚å¸¸===")
    print(e)
    
#ç”¨OutputFixingParserè‡ªåŠ¨ä¿®å¤å¹¶è§£æ
date = new_parser.parse(output)
print("===é‡æ–°è§£æç»“æœ===")
print(date)
```

    ===æ ¼å¼é”™è¯¯çš„Output===
    {"year": 2023, "month": å››æœˆ, "day": 6, "era": "AD"}
    ===å‡ºç°å¼‚å¸¸===
    Failed to parse Date from completion {"year": 2023, "month": å››æœˆ, "day": 6, "era": "AD"}. Got: Expecting value: line 1 column 25 (char 24)
    ===é‡æ–°è§£æç»“æœ===
    year=2023 month=4 day=6 era='AD'


<div class="alert alert-warning">
<b>æ€è€ƒï¼š</b>çŒœä¸€ä¸‹OutputFixingParseræ˜¯æ€ä¹ˆåšåˆ°çš„
</div>

### 1.4ã€å°ç»“

1. LangChain ç»Ÿä¸€å°è£…äº†å„ç§æ¨¡å‹çš„è°ƒç”¨æ¥å£ï¼ŒåŒ…æ‹¬è¡¥å…¨å‹å’Œå¯¹è¯å‹ä¸¤ç§
2. LangChain æä¾›äº† PromptTemplate ç±»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¸¦å˜é‡çš„æ¨¡æ¿
3. LangChain æä¾›äº†ä¸€äº›åˆ—è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡ï¼›é¢å¤–å¸¦æœ‰è‡ªåŠ¨ä¿®å¤åŠŸèƒ½ã€‚
4. ä¸Šè¿°æ¨¡å‹å±äº LangChain ä¸­è¾ƒä¸ºä¼˜ç§€çš„éƒ¨åˆ†ï¼›ç¾ä¸­ä¸è¶³çš„æ˜¯ OutputParser è‡ªèº«çš„ Prompt ç»´æŠ¤åœ¨ä»£ç ä¸­ï¼Œè€¦åˆåº¦è¾ƒé«˜ã€‚

## äºŒã€æ•°æ®è¿æ¥å°è£…

<img src="_images/llm/data_connection.jpg" style="margin-left: 0px" width=500px>

### 2.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders



```python
!pip install pypdf

```


```python
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("llama2.pdf")
pages = loader.load_and_split()

print(pages[0].page_content)
```

    Llama 2 : Open Foundation and Fine-Tuned Chat Models
    Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ 
    Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra
    Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen
    Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
    Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou
    Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev
    Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
    Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
    Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
    Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
    Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic
    Sergey Edunov Thomas Scialomâˆ—
    GenAI, Meta
    Abstract
    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
    large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
    Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our
    models outperform open-source chat models on most benchmarks we tested, and based on
    ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-
    source models. We provide a detailed description of our approach to fine-tuning and safety
    improvements of Llama 2-Chat in order to enable the community to build on our work and
    contribute to the responsible development of LLMs.
    âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com
    â€ Second author
    Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023


### 2.2 æ–‡æ¡£å¤„ç†å™¨

### 2.2.1 TextSplitter



```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=100,  # æ€è€ƒï¼šä¸ºä»€ä¹ˆè¦åšoverlap
    length_function=len,
    add_start_index=True,
)

paragraphs = text_splitter.create_documents([pages[0].page_content])
for para in paragraphs:
    print(para.page_content)
    print('-------')
```

    Llama 2 : Open Foundation and Fine-Tuned Chat Models
    Hugo Touvronâˆ—Louis Martinâ€ Kevin Stoneâ€ 
    Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra
    -------
    Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra
    Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen
    -------
    Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen
    Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
    -------
    Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller
    Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou
    -------
    Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou
    Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev
    -------
    Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev
    Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
    -------
    Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich
    Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
    -------
    Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra
    Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
    -------
    Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi
    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
    -------
    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang
    Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
    -------
    Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang
    Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic
    Sergey Edunov Thomas Scialomâˆ—
    -------
    Sergey Edunov Thomas Scialomâˆ—
    GenAI, Meta
    Abstract
    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
    -------
    Abstract
    In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
    large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
    -------
    large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
    Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our
    -------
    Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our
    models outperform open-source chat models on most benchmarks we tested, and based on
    -------
    models outperform open-source chat models on most benchmarks we tested, and based on
    ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-
    -------
    ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-
    source models. We provide a detailed description of our approach to fine-tuning and safety
    -------
    source models. We provide a detailed description of our approach to fine-tuning and safety
    improvements of Llama 2-Chat in order to enable the community to build on our work and
    -------
    improvements of Llama 2-Chat in order to enable the community to build on our work and
    contribute to the responsible development of LLMs.
    -------
    contribute to the responsible development of LLMs.
    âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com
    â€ Second author
    -------
    âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com
    â€ Second author
    Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023
    -------


<div class="alert alert-danger">
LangChain çš„ PDFLoader å’Œ TextSplitter å®ç°éƒ½æ¯”è¾ƒç²—ç³™ï¼Œå®é™…ç”Ÿäº§ä¸­ä¸å»ºè®®ä½¿ç”¨ã€‚
</div>

### 2.3ã€å†…ç½®çš„ RAG å®ç° 


```python
!pip install chromadb

```


```python
from langchain.document_loaders import UnstructuredMarkdownLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader

# åŠ è½½æ–‡æ¡£
loader = PyPDFLoader("llama2.pdf")
pages = loader.load_and_split()

# æ–‡æ¡£åˆ‡åˆ†
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300, 
    chunk_overlap=100,
    length_function=len,
    add_start_index=True,
)

texts = text_splitter.create_documents([pages[2].page_content,pages[3].page_content])

# çŒåº“
embeddings = OpenAIEmbeddings()
db = Chroma.from_documents(texts, embeddings)

# LangChainå†…ç½®çš„ RAG å®ç°
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(temperature=0), 
    retriever=db.as_retriever() 
)

query = "llama 2æœ‰å¤šå°‘å‚æ•°ï¼Ÿ"
response = qa_chain.invoke(query)
print(response["result"])
```

    Llama 2æœ‰7Bã€13Bå’Œ70Bå‚æ•°çš„å˜ä½“ã€‚


### 2.4ã€å°ç»“

1. è¿™éƒ¨åˆ†èƒ½åŠ› LangChain çš„å®ç°éå¸¸ç²—ç³™ï¼›
2. å®é™…ç”Ÿäº§ä¸­ï¼Œå»ºè®®è‡ªå·±å®ç°ï¼Œä¸å»ºè®®ç”¨ LangChain çš„å·¥å…·ã€‚

## ä¸‰ã€è®°å¿†å°è£…ï¼šMemory

### 3.1ã€å¯¹è¯ä¸Šä¸‹æ–‡ï¼šConversationBufferMemory



```python
from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory

history = ConversationBufferMemory()
history.save_context({"input": "ä½ å¥½å•Š"}, {"output": "ä½ ä¹Ÿå¥½å•Š"})

print(history.load_memory_variables({}))

history.save_context({"input": "ä½ å†å¥½å•Š"}, {"output": "ä½ åˆå¥½å•Š"})

print(history.load_memory_variables({}))
```

    {'history': 'Human: ä½ å¥½å•Š\nAI: ä½ ä¹Ÿå¥½å•Š'}
    {'history': 'Human: ä½ å¥½å•Š\nAI: ä½ ä¹Ÿå¥½å•Š\nHuman: ä½ å†å¥½å•Š\nAI: ä½ åˆå¥½å•Š'}


### 3.2ã€åªä¿ç•™ä¸€ä¸ªçª—å£çš„ä¸Šä¸‹æ–‡ï¼šConversationBufferWindowMemory



```python
from langchain.memory import ConversationBufferWindowMemory

window = ConversationBufferWindowMemory(k=1)
window.save_context({"input": "ç¬¬ä¸€è½®é—®"}, {"output": "ç¬¬ä¸€è½®ç­”"})
window.save_context({"input": "ç¬¬äºŒè½®é—®"}, {"output": "ç¬¬äºŒè½®ç­”"})
window.save_context({"input": "ç¬¬ä¸‰è½®é—®"}, {"output": "ç¬¬ä¸‰è½®ç­”"})
print(window.load_memory_variables({}))
```

    {'history': 'Human: ç¬¬ä¸‰è½®é—®\nAI: ç¬¬ä¸‰è½®ç­”'}


### 3.3ã€é€šè¿‡ Token æ•°æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼šConversationTokenBufferMemory



```python
from langchain.memory import ConversationTokenBufferMemory
from langchain_openai import ChatOpenAI

memory = ConversationTokenBufferMemory(
    llm=ChatOpenAI(),
    max_token_limit=40
)
memory.save_context(
    {"input": "ä½ å¥½å•Š"}, {"output": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ã€‚"})
memory.save_context(
    {"input": "ä½ ä¼šå¹²ä»€ä¹ˆ"}, {"output": "æˆ‘ä»€ä¹ˆéƒ½ä¼š"})

print(memory.load_memory_variables({}))
```

    {'history': 'Human: ä½ ä¼šå¹²ä»€ä¹ˆ\nAI: æˆ‘ä»€ä¹ˆéƒ½ä¼š'}


### 3.4ã€æ›´å¤šç±»å‹

- ConversationSummaryMemory: å¯¹ä¸Šä¸‹æ–‡åšæ‘˜è¦
  - https://python.langchain.com/docs/modules/memory/types/summary
- ConversationSummaryBufferMemory: ä¿å­˜ Token æ•°é™åˆ¶å†…çš„ä¸Šä¸‹æ–‡ï¼Œå¯¹æ›´æ—©çš„åšæ‘˜è¦
  - https://python.langchain.com/docs/modules/memory/types/summary_buffer
- VectorStoreRetrieverMemory: å°† Memory å­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ï¼Œæ ¹æ®ç”¨æˆ·è¾“å…¥æ£€ç´¢å›æœ€ç›¸å…³çš„éƒ¨åˆ†
  - https://python.langchain.com/docs/modules/memory/types/vectorstore_retriever_memory

### 3.5ã€å°ç»“

1. LangChain çš„ Memory ç®¡ç†æœºåˆ¶å±äºå¯ç”¨çš„éƒ¨åˆ†ï¼Œå°¤å…¶æ˜¯ç®€å•æƒ…å†µå¦‚æŒ‰è½®æ•°æˆ–æŒ‰ Token æ•°ç®¡ç†ï¼›
2. å¯¹äºå¤æ‚æƒ…å†µï¼Œå®ƒä¸ä¸€å®šæ˜¯æœ€ä¼˜çš„å®ç°ï¼Œä¾‹å¦‚æ£€ç´¢å‘é‡åº“æ–¹å¼ï¼Œå»ºè®®æ ¹æ®å®é™…æƒ…å†µå’Œæ•ˆæœè¯„ä¼°ï¼›
3. ä½†æ˜¯**å®ƒå¯¹å†…å­˜çš„å„ç§ç»´æŠ¤æ–¹æ³•çš„æ€è·¯åœ¨å®é™…ç”Ÿäº§ä¸­å¯ä»¥å€Ÿé‰´**ã€‚

## å››ã€Chain å’Œ LangChain Expression Language (LCEL)

LangChain Expression Languageï¼ˆLCELï¼‰æ˜¯ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œå¯è½»æ¾ç»„åˆä¸åŒçš„è°ƒç”¨é¡ºåºæ„æˆ Chainã€‚LCEL è‡ªåˆ›ç«‹ä¹‹åˆå°±è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿæ”¯æŒå°†åŸå‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œ**æ— éœ€ä»£ç æ›´æ”¹**ï¼Œä»æœ€ç®€å•çš„â€œæç¤º+LLMâ€é“¾åˆ°æœ€å¤æ‚çš„é“¾ï¼ˆå·²æœ‰ç”¨æˆ·æˆåŠŸåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„ LCEL Chainï¼‰ã€‚

LCELçš„ä¸€äº›äº®ç‚¹åŒ…æ‹¬ï¼š

1. **æµæ”¯æŒ**ï¼šä½¿ç”¨ LCEL æ„å»º Chain æ—¶ï¼Œä½ å¯ä»¥è·å¾—æœ€ä½³çš„é¦–ä¸ªä»¤ç‰Œæ—¶é—´ï¼ˆå³ä»è¾“å‡ºå¼€å§‹åˆ°é¦–æ‰¹è¾“å‡ºç”Ÿæˆçš„æ—¶é—´ï¼‰ã€‚å¯¹äºæŸäº› Chainï¼Œè¿™æ„å‘³ç€å¯ä»¥ç›´æ¥ä»LLMæµå¼ä¼ è¾“ä»¤ç‰Œåˆ°æµè¾“å‡ºè§£æå™¨ï¼Œä»è€Œä»¥ä¸ LLM æä¾›å•†è¾“å‡ºåŸå§‹ä»¤ç‰Œç›¸åŒçš„é€Ÿç‡è·å¾—è§£æåçš„ã€å¢é‡çš„è¾“å‡ºã€‚

2. **å¼‚æ­¥æ”¯æŒ**ï¼šä»»ä½•ä½¿ç”¨ LCEL æ„å»ºçš„é“¾æ¡éƒ½å¯ä»¥é€šè¿‡åŒæ­¥APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼‰å’Œå¼‚æ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangServe æœåŠ¡å™¨ä¸­ï¼‰è°ƒç”¨ã€‚è¿™ä½¿å¾—ç›¸åŒçš„ä»£ç å¯ç”¨äºåŸå‹è®¾è®¡å’Œç”Ÿäº§ç¯å¢ƒï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨åŒä¸€æœåŠ¡å™¨ä¸­å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚

3. **ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œ**ï¼šå½“ä½ çš„ LCEL é“¾æ¡æœ‰å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ­¥éª¤æ—¶ï¼ˆä¾‹å¦‚ï¼Œä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è·å–æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— è®ºæ˜¯åœ¨åŒæ­¥è¿˜æ˜¯å¼‚æ­¥æ¥å£ä¸­ï¼Œä»¥å®ç°æœ€å°çš„å»¶è¿Ÿã€‚

4. **é‡è¯•å’Œå›é€€**ï¼šä¸º LCEL é“¾çš„ä»»ä½•éƒ¨åˆ†é…ç½®é‡è¯•å’Œå›é€€ã€‚è¿™æ˜¯ä½¿é“¾åœ¨è§„æ¨¡ä¸Šæ›´å¯é çš„ç»ä½³æ–¹å¼ã€‚ç›®å‰æˆ‘ä»¬æ­£åœ¨æ·»åŠ é‡è¯•/å›é€€çš„æµåª’ä½“æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•å»¶è¿Ÿæˆæœ¬çš„æƒ…å†µä¸‹è·å¾—å¢åŠ çš„å¯é æ€§ã€‚

5. **è®¿é—®ä¸­é—´ç»“æœ**ï¼šå¯¹äºæ›´å¤æ‚çš„é“¾æ¡ï¼Œè®¿é—®åœ¨æœ€ç»ˆè¾“å‡ºäº§ç”Ÿä¹‹å‰çš„ä¸­é—´æ­¥éª¤çš„ç»“æœé€šå¸¸éå¸¸æœ‰ç”¨ã€‚è¿™å¯ä»¥ç”¨äºè®©æœ€ç»ˆç”¨æˆ·çŸ¥é“æ­£åœ¨å‘ç”Ÿä¸€äº›äº‹æƒ…ï¼Œç”šè‡³ä»…ç”¨äºè°ƒè¯•é“¾æ¡ã€‚ä½ å¯ä»¥æµå¼ä¼ è¾“ä¸­é—´ç»“æœï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªLangServeæœåŠ¡å™¨ä¸Šéƒ½å¯ç”¨ã€‚

6. **è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼**ï¼šè¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ä¸ºæ¯ä¸ª LCEL é“¾æä¾›äº†ä»é“¾çš„ç»“æ„æ¨æ–­å‡ºçš„ Pydantic å’Œ JSONSchema æ¨¡å¼ã€‚è¿™å¯ä»¥ç”¨äºè¾“å…¥å’Œè¾“å‡ºçš„éªŒè¯ï¼Œæ˜¯ LangServe çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚

7. **æ— ç¼LangSmithè·Ÿè¸ªé›†æˆ**ï¼šéšç€é“¾æ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç†è§£æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é€šè¿‡ LCELï¼Œæ‰€æœ‰æ­¥éª¤éƒ½è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œä»¥å®ç°æœ€å¤§çš„å¯è§‚å¯Ÿæ€§å’Œå¯è°ƒè¯•æ€§ã€‚

8. **æ— ç¼LangServeéƒ¨ç½²é›†æˆ**ï¼šä»»ä½•ä½¿ç”¨ LCEL åˆ›å»ºçš„é“¾éƒ½å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ LangServe è¿›è¡Œéƒ¨ç½²ã€‚

åŸæ–‡ï¼šhttps://python.langchain.com/docs/expression_language/

### çœ‹ä¸ªä¾‹å­


```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional
from enum import Enum
```


```python
# è¾“å‡ºç»“æ„
class SortEnum(str, Enum):
    data = 'data'
    price = 'price'

class OrderingEnum(str, Enum):
    ascend = 'ascend'
    descend = 'descend'

class Semantics(BaseModel):
    name: Optional[str] = Field(description="æµé‡åŒ…åç§°",default=None)
    price_lower: Optional[int] = Field(description="ä»·æ ¼ä¸‹é™",default=None)
    price_upper: Optional[int] = Field(description="ä»·æ ¼ä¸Šé™",default=None)
    data_lower: Optional[int] = Field(description="æµé‡ä¸‹é™",default=None)
    data_upper: Optional[int] = Field(description="æµé‡ä¸Šé™",default=None)
    sort_by: Optional[SortEnum] = Field(description="æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº",default=None)
    ordering: Optional[OrderingEnum] = Field(description="å‡åºæˆ–é™åºæ’åˆ—",default=None)

# OutputParser
parser = PydanticOutputParser(pydantic_object=Semantics)

# Prompt æ¨¡æ¿
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆJSONè¡¨ç¤ºã€‚è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š\n{format_instructions}\nä¸è¦è¾“å‡ºæœªæåŠçš„å­—æ®µã€‚",
        ),
        ("human", "{query}"),
    ]
).partial(format_instructions=parser.get_format_instructions())

# æ¨¡å‹
model = ChatOpenAI(temperature=0)

# LCEL è¡¨è¾¾å¼
runnable = (
    {"query": RunnablePassthrough()} | prompt | model | parser
)

# è¿è¡Œ
print(runnable.invoke("ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›"))
```

    name=None price_lower=None price_upper=100 data_lower=None data_upper=None sort_by=<SortEnum.data: 'data'> ordering=<OrderingEnum.descend: 'descend'>


### æ¢ä¸ªå¤æ‚ä¸€ç‚¹çš„ 

å›å¿† SK ä¸­çš„åµŒå¥—è°ƒç”¨


```python
from langchain_openai import OpenAIEmbeddings
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.vectorstores import Chroma

# å‘é‡æ•°æ®åº“
vectorstore = Chroma.from_texts(
    [
        "Sam Altmanæ˜¯OpenAIçš„CEO", 
        "Sam Altmanè¢«è§£é›‡äº†",
        "Sam Altmanè¢«å¤èŒäº†"
    ], embedding=OpenAIEmbeddings()
)

# æ£€ç´¢æ¥å£
retriever = vectorstore.as_retriever()

# Promptæ¨¡æ¿
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# Chain
retrieval_chain = (
    {"question": RunnablePassthrough(),"context": retriever}
    | prompt
    | model
    | StrOutputParser()
)

retrieval_chain.invoke("OpenAIçš„CEOæ˜¯è°")
```




    'OpenAIçš„CEOæ˜¯Sam Altmanã€‚'



**æ³¨æ„**: åœ¨å½“å‰çš„æ–‡æ¡£ä¸­ LCEL äº§ç”Ÿçš„å¯¹è±¡ï¼Œè¢«å«åš runnable æˆ– chainï¼Œç»å¸¸ä¸¤ç§å«æ³•æ··ç”¨ã€‚æœ¬è´¨å°±æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰è°ƒç”¨æµç¨‹ã€‚

<div class="alert alert-success">
<b>ä½¿ç”¨ LCEL çš„ä»·å€¼ï¼Œä¹Ÿå°±æ˜¯ LangChain çš„æ ¸å¿ƒä»·å€¼ã€‚</b> <br />
å®˜æ–¹ä»ä¸åŒè§’åº¦ç»™å‡ºäº†ä¸¾ä¾‹è¯´æ˜ï¼šhttps://python.langchain.com/docs/expression_language/why
</div>

### é€šè¿‡ LCELï¼Œè¿˜å¯ä»¥å®ç°

1. é…ç½®è¿è¡Œæ—¶å˜é‡ï¼šhttps://python.langchain.com/docs/expression_language/how_to/configure
2. æ•…éšœå›é€€ï¼šhttps://python.langchain.com/docs/expression_language/how_to/fallbacks
3. å¹¶è¡Œè°ƒç”¨ï¼šhttps://python.langchain.com/docs/expression_language/how_to/map
4. é€»è¾‘åˆ†æ”¯ï¼šhttps://python.langchain.com/docs/expression_language/how_to/routing
5. è°ƒç”¨è‡ªå®šä¹‰æµå¼å‡½æ•°ï¼šhttps://python.langchain.com/docs/expression_language/how_to/generators
6. é“¾æ¥å¤–éƒ¨Memoryï¼šhttps://python.langchain.com/docs/expression_language/how_to/message_history

æ›´å¤šä¾‹å­ï¼šhttps://python.langchain.com/docs/expression_language/cookbook/

<div class="alert alert-warning">
<b>æ€è€ƒï¼š</b>ä»æ¨¡å—é—´è§£ä¾èµ–è§’åº¦ï¼ŒLCELçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ
</div>

## äº”ã€æ™ºèƒ½ä½“æ¶æ„ï¼šAgent


### 5.1 å›å¿†ï¼šä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼ˆAgentï¼‰

å°†å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ä¸ªæ¨ç†å¼•æ“ã€‚ç»™å®šä¸€ä¸ªä»»åŠ¡ï¼Œæ™ºèƒ½ä½“è‡ªåŠ¨ç”Ÿæˆå®Œæˆä»»åŠ¡æ‰€éœ€çš„æ­¥éª¤ï¼Œæ‰§è¡Œç›¸åº”åŠ¨ä½œï¼ˆä¾‹å¦‚é€‰æ‹©å¹¶è°ƒç”¨å·¥å…·ï¼‰ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆã€‚

<img src="_images/llm/08agent-overview.png" style="margin-left: 0px" width=500px>

### 5.2 å…ˆå®šä¹‰ä¸€äº›å·¥å…·ï¼šTools

- å¯ä»¥æ˜¯ä¸€ä¸ªå‡½æ•°æˆ–ä¸‰æ–¹ API
- ä¹Ÿå¯ä»¥æŠŠä¸€ä¸ª Chain æˆ–è€… Agent çš„ run()ä½œä¸ºä¸€ä¸ª Tool



```python
from langchain import SerpAPIWrapper
from langchain.tools import Tool, tool

search = SerpAPIWrapper()
tools = [
    Tool.from_function(
        func=search.run,
        name="Search",
        description="useful for when you need to answer questions about current events"
    ),
]
```


```python
import calendar
import dateutil.parser as parser
from datetime import date

# è‡ªå®šä¹‰å·¥å…·
@tool("weekday")
def weekday(date_str: str) -> str:
    """Convert date to weekday name"""
    d = parser.parse(date_str)
    return calendar.day_name[d.weekday()]

tools += [weekday]
```

### 5.3 æ™ºèƒ½ä½“ç±»å‹ï¼šReAct

<img src="_images/llm/ReAct.png" style="margin-left: 0px" width=500px>



```python
!pip install google-search-results
!pip install langchainhub
```


```python
from langchain import hub
import json

# ä¸‹è½½ä¸€ä¸ªç°æœ‰çš„ Prompt æ¨¡æ¿
prompt = hub.pull("hwchase17/react")

print(prompt.template)
```

    Answer the following questions as best you can. You have access to the following tools:
    
    {tools}
    
    Use the following format:
    
    Question: the input question you must answer
    Thought: you should always think about what to do
    Action: the action to take, should be one of [{tool_names}]
    Action Input: the input to the action
    Observation: the result of the action
    ... (this Thought/Action/Action Input/Observation can repeat N times)
    Thought: I now know the final answer
    Final Answer: the final answer to the original input question
    
    Begin!
    
    Question: {input}
    Thought:{agent_scratchpad}



```python
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_react_agent


llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)

# å®šä¹‰ä¸€ä¸ª agent: éœ€è¦å¤§æ¨¡å‹ã€å·¥å…·é›†ã€å’Œ Prompt æ¨¡æ¿
agent = create_react_agent(llm, tools, prompt)
# å®šä¹‰ä¸€ä¸ªæ‰§è¡Œå™¨ï¼šéœ€è¦ agent å¯¹è±¡ å’Œ å·¥å…·é›†
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# æ‰§è¡Œ
agent_executor.invoke({"input": "å‘¨æ°ä¼¦ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå‡ "})
```


â€‹    
â€‹    [1m> Entering new AgentExecutor chain...[0m
â€‹    [32;1m[1;3mæˆ‘éœ€è¦çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥æ˜¯å“ªä¸€å¤©ï¼Œç„¶åæˆ‘å¯ä»¥ä½¿ç”¨weekdayå‡½æ•°æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚
â€‹    Action: Search
â€‹    Action Input: å‘¨æ°ä¼¦çš„ç”Ÿæ—¥[0m[36;1m[1;3mJanuary 18, 1979[0m[32;1m[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥æ˜¯1æœˆ18æ—¥ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨weekdayå‡½æ•°æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚
â€‹    Action: weekday
â€‹    Action Input: "1979-01-18"[0m[33;1m[1;3mThursday[0m[32;1m[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚
â€‹    Final Answer: æ˜ŸæœŸå››[0m
â€‹    
â€‹    [1m> Finished chain.[0m





    {'input': 'å‘¨æ°ä¼¦ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå‡ ', 'output': 'æ˜ŸæœŸå››'}



### 5.4 æ™ºèƒ½ä½“ç±»å‹ï¼šSelfAskWithSearch



```python
# ä¸‹è½½ä¸€ä¸ªæ¨¡æ¿
prompt = hub.pull("hwchase17/self-ask-with-search")

print(prompt.template)
```

    Question: Who lived longer, Muhammad Ali or Alan Turing?
    Are follow up questions needed here: Yes.
    Follow up: How old was Muhammad Ali when he died?
    Intermediate answer: Muhammad Ali was 74 years old when he died.
    Follow up: How old was Alan Turing when he died?
    Intermediate answer: Alan Turing was 41 years old when he died.
    So the final answer is: Muhammad Ali
    
    Question: When was the founder of craigslist born?
    Are follow up questions needed here: Yes.
    Follow up: Who was the founder of craigslist?
    Intermediate answer: Craigslist was founded by Craig Newmark.
    Follow up: When was Craig Newmark born?
    Intermediate answer: Craig Newmark was born on December 6, 1952.
    So the final answer is: December 6, 1952
    
    Question: Who was the maternal grandfather of George Washington?
    Are follow up questions needed here: Yes.
    Follow up: Who was the mother of George Washington?
    Intermediate answer: The mother of George Washington was Mary Ball Washington.
    Follow up: Who was the father of Mary Ball Washington?
    Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
    So the final answer is: Joseph Ball
    
    Question: Are both the directors of Jaws and Casino Royale from the same country?
    Are follow up questions needed here: Yes.
    Follow up: Who is the director of Jaws?
    Intermediate answer: The director of Jaws is Steven Spielberg.
    Follow up: Where is Steven Spielberg from?
    Intermediate answer: The United States.
    Follow up: Who is the director of Casino Royale?
    Intermediate answer: The director of Casino Royale is Martin Campbell.
    Follow up: Where is Martin Campbell from?
    Intermediate answer: New Zealand.
    So the final answer is: No
    
    Question: {input}
    Are followup questions needed here:{agent_scratchpad}



```python
from langchain.agents import create_self_ask_with_search_agent

tools = [
    Tool(
        name="Intermediate Answer",
        func=search.run,
        description="useful for when you need to ask with search.",
    )
]

# self_ask_with_search_agent åªèƒ½ä¼ ä¸€ä¸ªåä¸º 'Intermediate Answer' çš„ tool
agent = create_self_ask_with_search_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

agent_executor.invoke({"input": "å´äº¬çš„è€å©†ä¸»æŒè¿‡å“ªäº›ç»¼è‰ºèŠ‚ç›®"})
```


â€‹    
â€‹    [1m> Entering new AgentExecutor chain...[0m
â€‹    [32;1m[1;3mYes.
â€‹    Follow up: Who is å´äº¬'s wife?[0m[36;1m[1;3m['ç®€ä»‹ï¼š ä½ çŸ¥é“å´äº¬å¨¶è¿‡å‡ ä¸ªè€å©†å—åªç»“äº†ä¸€æ¬¡å©šç›®å‰ä»–æœ‰ä¸¤ä¸ªå­©å­å´äº¬çš„è€å©†æ˜¯è°¢æ¥ ä»–ä»¬æ˜¯åœ¨2014å¹´ç»“çš„å©š2018å¹´ç”Ÿçš„ç¬¬äºŒä¸ªå­©å­å´è™‘1974å¹´4... å°æå­çœŸå®å½±åƒ.', "Li Bingbing's first-time in an English-language film is Wayne Wang's Snow Flower ... ^ ç¦å¸ƒæ–¯ä¸­å›½å‘å¸ƒ100åäººæ¦œ å´äº¬é»„æ¸¤èƒ¡æ­Œä½åˆ—å‰ä¸‰ . Sina Entertainment (in ...", 'å´äº¬å¾ˆéœ¸æ°”çš„æ‹’ç»äº†å¥¹çš„è¦æ±‚ï¼Œè¡¨ç¤ºä»–åªé€‰å¯¹çš„äººï¼Œè€Œä¸é€‰è´µçš„äººã€‚çœ‹åˆ°è¿™é‡Œï¼Œå¾ˆå¤šç½‘å‹å°±è¦é«˜æ½®äº†ï¼Œâ€œå“¼ï¼Œè¿™äº›æµé‡æ˜æ˜Ÿæ‹½ä»€ä¹ˆæ‹½ï¼Œç°åœ¨è‚ å­éƒ½æ‚”é’äº†å§ï¼â€.', 'Comments Â· é«˜æ¸…å¤§Sï¼Œå´äº¬ï¼Œè‚è¿œç‰ˆï¼ˆç¬¬34é›†ï¼‰ Â· å€©å¥³å¹½é­‚å´äº¬ç‰ˆå¹•åèŠ±çµ® Â· Ending Chapter! Â· CEO attended party with mistress in high-profile, wife ...', 'æ˜æ˜Ÿè®¿è°ˆä¸€æ¡£å®£æ‰¬æ€åº¦çš„æ˜æ˜Ÿè®¿è°ˆæ–°ç»¼è‰ºï¼Œäº¦åŠ¨äº¦é™çš„å¯¹å˜‰å®¾åŠ±å¿—æ•…äº‹åšæ·±åº¦å‰–æï¼Œå…¨æ–¹ä½ã€çœŸå®ã€ç«‹ä½“åœ°å±•ç°å˜‰å®¾çš„å½¢è±¡ã€æ€§æ ¼ï¼Œå±•ç°å˜‰å®¾é²œæ´»çœŸå®çš„ä¸€é¢ã€‚', 'Lixiaopeng&#39;s wife, Zhou Yangqing, is a retired Olympic champion gymnast from China. She won a gold medal in the uneven bars event at the ...', 'ã€Šå½±è§†é£äº‘ã€‹æ ç›®æ˜¯åŒ—äº¬ç”µè§†å°å”¯ä¸€ä¸€æ¡£å¤§å‹å½±è§†è®¿è°ˆèŠ‚ç›®ã€‚ä»¥å›é¡¾ç»å…¸ä¼˜ç§€å½±è§†ä½œå“ã€å®£ä¼ æ¨èå„é¢‘é“çƒ­æ’­ç”µè§†å‰§åŠè¿½è¸ªå›½å†…å³å°†ä¸Šæ˜ ç”µå½±ä¸ºä¸»è¦å†…å®¹ï¼Œ ...', "Esther's wife Â· æå‰ç»™å¥³å„¿åšæ•°æ®å™œ#è™ä¹¦æ¬£#è™ä¹¦æ¬£æ°¸å¤œæ˜Ÿæ²³#è™ä¹¦æ¬£å°. 13.0 ... å´äº¬å‡ ä¸ªå­©å­. 7454. 00:00 Â· å´äº¬å‡ ä¸ªå­©å­ Â· @ ç æ±Ÿè§†é¢‘ Â· å§œå¦ç»“å©šäº†å—.", 'ã€FULLã€‘å´äº¬è°¢æ¥ å¤«å¦‡ä¹˜åç”œèœœå†’é™©ä¸“è½¦æˆ˜ç‹¼é“æ±‰æŸ”æƒ…å°½æ˜¾åå·®èŒã€ŠçœŸæ˜Ÿè¯å¤§å†’é™©ã€‹ç¬¬12æœŸ20170724[æµ™æ±Ÿå«è§†å®˜æ–¹HD]. 249K views Â· 6 years ago ...more ...'][0m[32;1m[1;3mFollow up: What variety shows has è°¢æ¥  hosted?[0m[36;1m[1;3m['TBA, Love Actually Season 3 add. Chinese TV Show, 0000, 10 eps. (Main Host). 10 ; 2023, Ace vs Ace Season 8 add. Chinese TV Show, 2023, 12 eps. (Ep. 1) (Guest).', 'ç”Ÿå¹³ äº2005å¹´â€œçŒ«äººè¶…çº§é­…åŠ›ä¸»æŒç§€â€å† å†›è„±é¢–è€Œå‡ºï¼Œç°ä»»å…‰çº¿ä¼ åª’æ——ä¸‹ä¸»æ‰“èŠ‚ç›®ã€Šå¨±ä¹ç°åœºã€‹ã€ã€Šæœ€ä½³ç°åœºã€‹ã€ã€Šå½±è§†é£äº‘æ¦œã€‹å½“å®¶ä¸»æŒã€‚ 2011å¹´11æœˆ24æ—¥,è°¢æ¥ å‘è¡Œé¦–å¼ ä¸ªäººepã€Šæœ€å¥½çš„æˆ‘ä»¬ã€‹ã€‚ 2014å¹´ï¼Œå´äº¬å‘å¸ƒæ–°å¹´å¾®åšå…¬å¸ƒå©šè®¯ï¼Œè¡¨ç¤ºå·²ç»ä¸è°¢æ¥ ç»“å©šã€‚', '2016å¹´ï¼Œä¸»æ¼”çš„å¥‡å¹»ç‰‡ã€Šå¤§è¯è¥¿æ¸¸3ã€‹ä¸Šæ˜ ã€‚ 2017å¹´ï¼Œä¸»æ¼”ç”µå½±ã€Šè¿™ä½å£®å£«ã€‹ã€‚ 2019å¹´ï¼Œåœ¨ç¾é£ŸçœŸäººç§€ã€Šç†Ÿæ‚‰çš„å‘³é“ç¬¬å››å­£ã€‹ä¸­æ‹…ä»»ä¸»æŒäººã€‚ 2020å¹´ï¼Œä¸»æŒçš„åœºæ™¯é—¯å…³å¼äººç‰©è®¿è°ˆèŠ‚ç›®ã€Šè¿½æ¢¦äººä¹‹å¼€åˆäººç”Ÿã€‹æ’­å‡ºï¼›åŒå¹´ï¼Œä½œä¸ºå¸¸é©»å˜‰å®¾å‚åŠ å®æ™¯è§‚å¯ŸèŠ‚ç›®ã€Šå¹¸ç¦ä¸‰é‡å¥ç¬¬ä¸‰å­£ã€‹ã€‚', 'Xie Nan (è°¢æ¥ ) was born on November 6, 1983. Xie Nan movies and tv shows: After Love Actually 2022 (China), Snow Day 2022 (China).', '... has been held for thousands of years. Onentert New 700 views Â· 5:38 Â· Go ... Welcome Back To Sound EP5ã€èŠ’æœTVçˆ±è±†å¨±ä¹ç«™ã€‘. èŠ’æœTVçˆ±è±†MangoTV Idol ...', 'ä¸­å›½å†…åœ°å¥³ä¸»æŒäººã€æ¼”å‘˜.', 'The sixth season of the Chinese reality talent show Sing! China premiered on 30 July 2021, on Zhejiang Television. Li Ronghao returned as coach for his ...', 'to perform. It has then transformed to an election show to choose the current show hosts. The current show model use Interviews and games ...', 'The couple joined the cast of Chinese variety show, â€œHappiness Trio 3â€ (lit. å¹¸ç¦ä¸‰é‡å¥3), as one of three married couples revealing their ...', '... TV is an entertainment reality show aired since July 1997. The show often invites grassroots including kids with talent to perform. It has ...'][0m[32;1m[1;3mSo the final answer is: è°¢æ¥  has hosted shows like "å¨±ä¹ç°åœº", "æœ€ä½³ç°åœº", "å½±è§†é£äº‘æ¦œ", "ç†Ÿæ‚‰çš„å‘³é“ç¬¬å››å­£", and "è¿½æ¢¦äººä¹‹å¼€åˆäººç”Ÿ".[0m
â€‹    
â€‹    [1m> Finished chain.[0m





    {'input': 'å´äº¬çš„è€å©†ä¸»æŒè¿‡å“ªäº›ç»¼è‰ºèŠ‚ç›®',
     'output': 'è°¢æ¥  has hosted shows like "å¨±ä¹ç°åœº", "æœ€ä½³ç°åœº", "å½±è§†é£äº‘æ¦œ", "ç†Ÿæ‚‰çš„å‘³é“ç¬¬å››å­£", and "è¿½æ¢¦äººä¹‹å¼€åˆäººç”Ÿ".'}



### 5.5 OpenAI Assistants


```python
from langchain.agents.openai_assistant import OpenAIAssistantRunnable

interpreter_assistant = OpenAIAssistantRunnable.create_assistant(
    name="langchain assistant",
    instructions="You are a personal math tutor. Write and run code to answer math questions.",
    tools=[{"type": "code_interpreter"}],
    model="gpt-3.5-turbo",
)
output = interpreter_assistant.invoke({"content": "10å‡4çš„å·®çš„2.3æ¬¡æ–¹æ˜¯å¤šå°‘"})

print(output[0].content[0].text.value)
```

    10å‡4çš„å·®çš„2.3æ¬¡æ–¹æ˜¯61.62ã€‚


<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>
<ol>
<li>ReAct æ˜¯æ¯”è¾ƒå¸¸ç”¨çš„ Planner</li>
<li>SelfAskWithSearch æ›´é€‚åˆéœ€è¦å±‚å±‚æ¨ç†çš„åœºæ™¯ï¼ˆä¾‹å¦‚çŸ¥è¯†å›¾è°±ï¼‰</li>
<li>OpenAI Assistants ä¸æ˜¯ä¸‡èƒ½çš„ï¼ŒLangChain çš„å®˜æ–¹æ–‡æ¡£é‡Œä¹Ÿä¸å¼ºè°ƒè¿°æ¥å£äº†</li>
<li>Agentè½åœ°åº”ç”¨éœ€è¦æ›´å¤šç»†èŠ‚ï¼Œåé¢è¯¾ç¨‹ä¸­æˆ‘ä»¬ä¼šä¸“é—¨è®² Agent çš„å®ç°</li>
</ol>
</div>

## å…­ã€LangServe

LangServe ç”¨äºå°† Chain æˆ–è€… Runnable éƒ¨ç½²æˆä¸€ä¸ª REST API æœåŠ¡ã€‚


```python
# å®‰è£… LangServe
!pip install "langserve[all]"

# ä¹Ÿå¯ä»¥åªå®‰è£…ä¸€ç«¯
# !pip install "langserve[client]"
# !pip install "langserve[server]"
```

### 6.1ã€Serverç«¯

```python
#!/usr/bin/env python
from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langserve import add_routes
import uvicorn

app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple api server using Langchain's Runnable interfaces",
)

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯")
add_routes(
    app,
    prompt | model,
    path="/joke",
)

if __name__ == "__main__":
    uvicorn.run(app, host="localhost", port=9999)
```

### 6.2ã€Clientç«¯

```python
import requests



response = requests.post(

    "http://localhost:9999/joke/invoke",

    json={'input': {'topic': 'å°æ˜'}}

)

print(response.json())
```

## ä¸ƒã€LangChain.js

Python ç‰ˆ LangChain çš„å§Šå¦¹é¡¹ç›®ï¼Œéƒ½æ˜¯ç”± Harrison Chase ä¸»ç†ã€‚

é¡¹ç›®åœ°å€ï¼šhttps://github.com/langchain-ai/langchainjs

æ–‡æ¡£åœ°å€ï¼šhttps://js.langchain.com/docs/

ç‰¹è‰²ï¼š

1. å¯ä»¥å’Œ Python ç‰ˆ LangChain æ— ç¼å¯¹æ¥

2. æŠ½è±¡è®¾è®¡å®Œå…¨ç›¸åŒï¼Œæ¦‚å¿µä¸€ä¸€å¯¹åº”

3. æ‰€æœ‰å¯¹è±¡åºåˆ—åŒ–åéƒ½èƒ½è·¨è¯­è¨€ä½¿ç”¨ï¼Œä½† API å·®åˆ«æŒºå¤§ï¼Œä¸è¿‡åœ¨åŠªåŠ›å¯¹é½

æ”¯æŒç¯å¢ƒï¼š

1. Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x
2. Cloudflare Workers
3. Vercel / Next.js (Browser, Serverless and Edge functions)
4. Supabase Edge Functions
5. Browser
6. Deno

å®‰è£…ï¼š
```
npm install langchain
```

å½“å‰é‡ç‚¹ï¼š

1. è¿½ä¸Š Python ç‰ˆçš„èƒ½åŠ›ï¼ˆç”šè‡³ä¸ºæ­¤åšäº†ä¸€ä¸ªåŸºäº gpt-3.5-turbo çš„ä»£ç ç¿»è¯‘å™¨ï¼‰
2. ä¿æŒå…¼å®¹å°½å¯èƒ½å¤šçš„ç¯å¢ƒ
3. å¯¹è´¨é‡å…³æ³¨ä¸å¤šï¼Œéšæ—¶é—´è‡ªç„¶èƒ½è§£å†³

## LangChain ä¸ Semantic Kernel å¯¹æ¯”

| åŠŸèƒ½/å·¥å…·           | LangChain                       | Semantic Kernel                  |
|-------------------|:---------------------------------:|:----------------------------------:|
| ç‰ˆæœ¬å·        |  0.1.0  | python-0.4.4.dev  |
| é€‚é…çš„ LLM        | å¤š   | å°‘ + å¤–éƒ¨ç”Ÿæ€   |
| Prompt å·¥å…·        | æ”¯æŒ    | æ”¯æŒ     |
| Prompt å‡½æ•°åµŒå¥—    | éœ€è¦é€šè¿‡ LCEL | æ”¯æŒ        |
| Prompt æ¨¡æ¿åµŒå¥—    | ä¸æ”¯æŒ  | ä¸æ”¯æŒ       |
| è¾“å‡ºè§£æå·¥å…·       | æ”¯æŒ  | ä¸æ”¯æŒ  |
| ä¸Šä¸‹æ–‡ç®¡ç†å·¥å…·           | æ”¯æŒ | C#ç‰ˆæ”¯æŒï¼ŒPythonç‰ˆå°šæœªæ”¯æŒ  |
| å†…ç½®å·¥å…·           | å¤šï¼Œä½†è‰¯è ä¸é½  | å°‘ + å¤–éƒ¨ç”Ÿæ€  |
| ä¸‰æ–¹å‘é‡æ•°æ®åº“é€‚é…           | å¤š | å°‘ + å¤–éƒ¨ç”Ÿæ€  |
| æœåŠ¡éƒ¨ç½² | LangServe | ä¸ Azure è¡”æ¥æ›´ä¸æ»‘
| ç®¡ç†å·¥å…· | LangSmith/LangFuse | Prompt Flow

## æ€»ç»“

1. LangChain éšç€ç‰ˆæœ¬è¿­ä»£å¯ç”¨æ€§æœ‰æ˜æ˜¾æå‡
2. ä½¿ç”¨ LangChain è¦é¿å¼€å­˜åœ¨å¤§é‡ä»£ç å†… Prompt çš„æ¨¡å—
3. å®ƒçš„å†…ç½®åŸºç¡€å·¥å…·ï¼Œå»ºè®®å……åˆ†æµ‹è¯•æ•ˆæœåå†å†³å®šæ˜¯å¦ä½¿ç”¨

## ä½œä¸š


ç”¨ LangChain é‡æ„ ChatPDF çš„ä½œä¸š
