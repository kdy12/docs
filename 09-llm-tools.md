# ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ 

1. ç³»ç»Ÿæ€§ç»´æŠ¤ã€æµ‹è¯•ã€ç›‘æ§ä¸€ä¸ª LLM åº”ç”¨
2. å­¦ä¹ ä½¿ç”¨ä¸»æµçš„å·¥å…·å®Œæˆä¸Šè¿°å·¥ä½œ

## ç»´æŠ¤ä¸€ä¸ªç”Ÿäº§çº§çš„ LLM åº”ç”¨ï¼Œæˆ‘ä»¬éœ€è¦åšä»€ä¹ˆï¼Ÿ

1. å„ç§æŒ‡æ ‡ç›‘æ§ä¸ç»Ÿè®¡ï¼šè®¿é—®è®°å½•ã€å“åº”æ—¶é•¿ã€Tokenç”¨é‡ã€è®¡è´¹ç­‰ç­‰
2. è°ƒè¯• Prompt
3. æµ‹è¯•/éªŒè¯ç³»ç»Ÿçš„ç›¸å…³è¯„ä¼°æŒ‡æ ‡
4. æ•°æ®é›†ç®¡ç†ï¼ˆä¾¿äºå›å½’æµ‹è¯•ï¼‰
5. Prompt ç‰ˆæœ¬ç®¡ç†ï¼ˆä¾¿äºå‡çº§/å›æ»šï¼‰

## é’ˆå¯¹ä»¥ä¸Šéœ€æ±‚ï¼Œæˆ‘ä»¬ä»‹ç»ä¸‰ä¸ªç”Ÿäº§çº§ LLM App ç»´æŠ¤å¹³å°

1. é‡ç‚¹è®²è§£ **LangFuse**: å¼€æº + SaaSï¼ŒLangSmith å¹³æ›¿ï¼Œå¯é›†æˆ LangChain ä¹Ÿå¯ç›´æ¥å¯¹æ¥ OpenAI APIï¼›
2. ç®€å•è®²è§£ **LangSmith**: LangChain çš„å®˜æ–¹å¹³å°ï¼ŒSaaS æœåŠ¡ï¼Œéå¼€æºï¼Œ**ç›®å‰éœ€è¦æ’é˜Ÿæ³¨å†Œ**ï¼›
3. ç®€å•è®²è§£ **Prompt Flow**ï¼šå¾®è½¯å¼€å‘ï¼Œå¼€æº + Azure AIäº‘æœåŠ¡ï¼Œå¯é›†æˆ Semantic Kernelï¼ˆä½†è²Œåˆç¥ç¦»ï¼‰ã€‚

## 1ã€LangFuse

å¼€æºï¼Œæ”¯æŒ LangChain é›†æˆæˆ–åŸç”Ÿ OpenAI API é›†æˆ

å®˜æ–¹ç½‘ç«™ï¼šhttps://langfuse.com/

é¡¹ç›®åœ°å€ï¼šhttps://github.com/langfuse

1. é€šè¿‡å®˜æ–¹äº‘æœåŠ¡ä½¿ç”¨ï¼š
   - æ³¨å†Œ: cloud.langfuse.com
   - åˆ›å»º API Key

```sh
LANGFUSE_SECRET_KEY="sk-lf-..."
LANGFUSE_PUBLIC_KEY="pk-lf-..."
```

2. é€šè¿‡ Docker æœ¬åœ°éƒ¨ç½²

```sh
# Clone repository
git clone https://github.com/langfuse/langfuse.git
cd langfuse
 
# Run server and db
docker compose up -d
```


```python
!pip install --upgrade langfuse
```

### 1.1ã€æ›¿æ¢ OpenAI å®¢æˆ·ç«¯


```python
from datetime import datetime
from langfuse.openai import openai
from langfuse import Langfuse 
import os

trace = Langfuse().trace(
    name = "hello-world",
    user_id = "wzr",
    release = "v0.0.1"
)

completion = openai.chat.completions.create(
  name="hello-world",
  model="gpt-3.5-turbo",
  messages=[
      {"role": "user", "content": "å¯¹æˆ‘è¯´'Hello, World!'"}
  ],
  temperature=0,
  trace_id=trace.id,
)

print(completion.choices[0].message.content)

```

    Hello, World!


### 1.2ã€é€šè¿‡ LangChain çš„å›è°ƒé›†æˆ


```python
from langfuse.callback import CallbackHandler

handler = CallbackHandler(
    trace_name="SayHello",
    user_id="wzr",
)
```


```python
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

#from langchain.chat_models import ErnieBotChat
from langchain.schema import HumanMessage
from langchain.prompts.chat import HumanMessagePromptTemplate
from langchain.prompts import ChatPromptTemplate

model = ChatOpenAI(model="gpt-3.5-turbo-0613")

prompt = ChatPromptTemplate.from_messages([
    HumanMessagePromptTemplate.from_template("Say hello to {input}!") 
])


# å®šä¹‰è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

chain = (
    {"input":RunnablePassthrough()} 
    | prompt
    | model
    | parser
)
```


```python
chain.invoke(input="AGIClass", config={"callbacks":[handler]})
```

    ERROR:langchain_core.tracers.langchain:Authentication failed for https://api.smith.langchain.com/runs. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs', '{"detail":"Invalid auth"}')
    ERROR:langchain_core.tracers.langchain:Authentication failed for https://api.smith.langchain.com/runs/6cb93a2c-2200-4062-8218-a20ac7f27753. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/6cb93a2c-2200-4062-8218-a20ac7f27753', '{"detail":"Invalid auth"}')





    'Hello AGIClass!'



### 1.3ã€æ„å»ºä¸€ä¸ªå®é™…åº”ç”¨

**AGIè¯¾å ‚è·Ÿè¯¾åŠ©æ‰‹**ï¼Œæ ¹æ®è¯¾ç¨‹å†…å®¹ï¼Œåˆ¤æ–­å­¦ç”Ÿé—®é¢˜æ˜¯å¦éœ€è¦è€å¸ˆè§£ç­”

1. åˆ¤æ–­è¯¥é—®é¢˜æ˜¯å¦éœ€è¦è€å¸ˆè§£ç­”ï¼Œå›å¤'Y'æˆ–'N'
2. åˆ¤æ–­è¯¥é—®é¢˜æ˜¯å¦å·²æœ‰åŒå­¦é—®è¿‡


```python
# æ„å»º PromptTemplate
from langchain.prompts import PromptTemplate

need_answer=PromptTemplate.from_template("""
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
{outlines}
*********
å­¦å‘˜è¾“å…¥:
{user_input}
*********
å¦‚æœè¿™æ˜¯ä¸€ä¸ªéœ€è¦è€å¸ˆç­”ç–‘çš„é—®é¢˜ï¼Œå›å¤Yï¼Œå¦åˆ™å›å¤Nã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚""")

check_duplicated=PromptTemplate.from_template("""
*********
å·²æœ‰æé—®åˆ—è¡¨:
[
{question_list}
]
*********
æ–°æé—®:
{user_input}
*********
å·²æœ‰æé—®åˆ—è¡¨æ˜¯å¦æœ‰å’Œæ–°æé—®ç±»ä¼¼çš„é—®é¢˜? å›å¤Yæˆ–N, Yè¡¨ç¤ºæœ‰ï¼ŒNè¡¨ç¤ºæ²¡æœ‰ã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚""")
```


```python
outlines="""
LangChain
æ¨¡å‹ I/O å°è£…
æ¨¡å‹çš„å°è£…
æ¨¡å‹çš„è¾“å…¥è¾“å‡º
PromptTemplate
OutputParser
æ•°æ®è¿æ¥å°è£…
æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders
æ–‡æ¡£å¤„ç†å™¨
å†…ç½®RAGï¼šRetrievalQA
è®°å¿†å°è£…ï¼šMemory
é“¾æ¶æ„ï¼šChain/LCEL
å¤§æ¨¡å‹æ—¶ä»£çš„è½¯ä»¶æ¶æ„ï¼šAgent
ReAct
SelfAskWithSearch
Assistants API
LangServe
LangChain.js
"""

question_list=[
    "è°¢è°¢è€å¸ˆ",
    "LangChainå¼€æºå—",
]
```


```python
# åˆ›å»º chain
model = ChatOpenAI(temperature=0,model_kwargs={"seed":42})
parser = StrOutputParser()

chain1 = (
    need_answer
    | model
    | parser
)

chain2 = (
    check_duplicated
    | model
    | parser
)
```

### 1.3.1ã€ç”¨ Trace è®°å½•ä¸€ä¸ªå¤šæ¬¡è°ƒç”¨ LLM çš„è¿‡ç¨‹
TRACE (id: trace_id)
|
|-- SPAN: LLMCain (id: generated by Langfuse)
|   |
|   |-- GENERATION: OpenAI (id: generated by Langfuse)
|
|-- SPAN: LLMCain (id: generated by 'next_span_id')
|   |
|   |-- GENERATION: OpenAI (id: generated by Langfuse)

```python
import uuid
from langfuse.client import Langfuse

# åˆ›å»ºä¸€ä¸ªæ–°trace
def create_trace(user_id):
    langfuse = Langfuse()
    trace_id = str(uuid.uuid4())
    trace = langfuse.trace(
        name="assistant",
        id=trace_id,
        user_id=user_id
    )
    return trace

# ä¸»æµç¨‹
def verify_question(
    question: str,
    outlines: str,
    question_list: list,
    user_id: str,
) -> bool:
    trace = create_trace(user_id)
    handler = trace.get_langchain_handler()
    # åˆ¤æ–­æ˜¯å¦éœ€è¦å›ç­”
    if chain1.invoke(
        {"user_input":question,"outlines": outlines},
        config={"callbacks":[handler]}
    ) == 'Y':
        # åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤é—®é¢˜
        if chain2.invoke(
            {"user_input":question,"question_list": "\n".join(question_list)},
            config={"callbacks":[handler]}
        ) == 'N':
            question_list.append(question)
            return True
    return False
```


```python
# å®é™…è°ƒç”¨
ret = verify_question(
    #"LangChainå’ŒSKå“ªä¸ªå¥½ç”¨",
    "LangChainæ”¯æŒJavaå—",
    outlines,
    question_list,
    user_id="wzr",
)
print(ret)
```

    True


### 1.3.2ã€ç”¨ Session è®°å½•ä¸€ä¸ªç”¨æˆ·çš„å¤šè½®å¯¹è¯
SESSION (id: session_id)
|
|-- TRACE
|-- TRACE
|-- TRACE
|-- ...

```python
import uuid
from langchain_openai import ChatOpenAI
from langchain.schema import (
    AIMessage, #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role
    HumanMessage, #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role
    SystemMessage #ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role
)


llm = ChatOpenAI()

messages = [
    SystemMessage(content="ä½ æ˜¯AGIClassçš„è¯¾ç¨‹åŠ©ç†ã€‚"), 
]

handler = CallbackHandler(
    user_id="wzr",
    trace_name="test_chat",
    session_id=str(uuid.uuid4())
)

while True:
    user_input=input("User: ")
    if user_input.strip() == "":
        break
    messages.append(HumanMessage(content=user_input))
    response = llm.invoke(messages,config={"callbacks":[handler]})
    print("AI: "+response.content)
    messages.append(response)
```

    User:  ä½ å¥½


    AI: ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘èƒ½å¸®å¿™çš„å—ï¼Ÿ


    User:  ä½ æ˜¯è°


    AI: æˆ‘æ˜¯ä¸€ä¸ªAIç¨‹åºï¼Œè¢«ç§°ä¸ºAGIClassçš„è¯¾ç¨‹åŠ©ç†ã€‚æˆ‘è¢«è®¾è®¡ç”¨æ¥å¸®åŠ©æä¾›å…³äºAGIClassè¯¾ç¨‹çš„ä¿¡æ¯å’Œç­”ç–‘è§£æƒ‘ã€‚æœ‰ä»€ä¹ˆæˆ‘èƒ½å¸®åˆ°ä½ çš„å—ï¼Ÿ


    User:  


### 1.4ã€æ•°æ®é›†ä¸æµ‹è¯•

### 1.4.1ã€åœ¨çº¿æ ‡æ³¨

<img src="_images/llm/annotation.png" width="600px">

### 1.4.2ã€ä¸Šä¼ å·²æœ‰æ•°æ®é›†


```python
import json

data = []
with open('my_annotations.jsonl','r',encoding='utf-8') as fp:
    for line in fp:
        example = json.loads(line.strip())
        item = {
            "input": {
                "outlines": example["outlines"],
                "user_input": example["user_input"]
            },
            "expected_output": example["label"]
        }
        data.append(item)
```


```python
from langfuse import Langfuse
from langfuse.model import CreateDatasetRequest, CreateDatasetItemRequest
from tqdm import tqdm

# init
langfuse = Langfuse()

# è€ƒè™‘æ¼”ç¤ºè¿è¡Œé€Ÿåº¦ï¼Œåªä¸Šä¼ å‰50æ¡æ•°æ®
for item in tqdm(data[:50]):
    langfuse.create_dataset_item(
        dataset_name="teacher-assistant",
        input=item["input"],
        expected_output=item["expected_output"]
    )
```

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:16<00:00,  3.04it/s]


### 1.4.3ã€å®šä¹‰è¯„ä¼°å‡½æ•°


```python
def simple_evaluation(output, expected_output):
  return output == expected_output
```

### 1.4.4ã€è¿è¡Œæµ‹è¯•

Prompt æ¨¡æ¿ä¸ Chainï¼ˆLCELï¼‰


```python
from langchain.prompts import PromptTemplate

need_answer=PromptTemplate.from_template("""
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
{outlines}
*********
å­¦å‘˜è¾“å…¥:
{user_input}
*********
å¦‚æœè¿™æ˜¯ä¸€ä¸ªéœ€è¦è€å¸ˆç­”ç–‘çš„é—®é¢˜ï¼Œå›å¤Yï¼Œå¦åˆ™å›å¤Nã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚""")

model = ChatOpenAI(temperature=0,model_kwargs={"seed":42})
parser = StrOutputParser()

chain_v1 = (
    need_answer
    | model
    | parser
)
```

åœ¨æ•°æ®é›†ä¸Šæµ‹è¯•æ•ˆæœ


```python
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from langfuse import Langfuse

langfuse = Langfuse()

def run_evaluation(chain, dataset_name, run_name):
    dataset = langfuse.get_dataset(dataset_name)

    def process_item(item):
        handler = item.get_langchain_handler(run_name=run_name)
        
        # Assuming chain.invoke is a synchronous function
        output = chain.invoke(item.input, config={"callbacks": [handler]})
        
        # Assuming handler.root_span.score is a synchronous function
        handler.root_span.score(
            name="accuracy",
            value=simple_evaluation(output, item.expected_output)
        )
        print('.', end='',flush=True)

    # Using ThreadPoolExecutor with a maximum of 10 workers
    with ThreadPoolExecutor(max_workers=4) as executor:
        # Map the process_item function to each item in the dataset
        executor.map(process_item, dataset.items)
```


```python
run_evaluation(chain_v1, "teacher-assistant", "v1-"+str(uuid.uuid4())[:8])
```

### 1.4.5ã€Prompt è°ƒä¼˜ä¸å›å½’æµ‹è¯•

ä¼˜åŒ– Promptï¼šè¯•è¯•æ€ç»´é“¾ï¼ˆå›å¿†[ç¬¬äºŒè¯¾](../02-prompt/index.ipynb)ï¼‰


```python
from langchain.prompts import PromptTemplate

need_answer=PromptTemplate.from_template("""
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚

ä½ çš„é€‰æ‹©éœ€è¦éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1 éœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜æ˜¯æŒ‡ä¸è¯¾ç¨‹å†…å®¹æˆ–AI/LLMç›¸å…³çš„æŠ€æœ¯é—®é¢˜ï¼›
2 è¯„è®ºæ€§çš„è§‚ç‚¹ã€é—²èŠã€è¡¨è¾¾æ¨¡ç³Šä¸æ¸…çš„å¥å­ï¼Œä¸éœ€è¦è€å¸ˆå›ç­”ï¼›
3 å­¦ç”Ÿè¾“å…¥ä¸æ„æˆç–‘é—®å¥çš„ï¼Œä¸éœ€è¦è€å¸ˆå›ç­”ï¼›
4 å­¦ç”Ÿé—®é¢˜ä¸­å¦‚æœç”¨â€œè¿™â€ã€â€œé‚£â€ç­‰ä»£è¯æŒ‡ä»£ï¼Œä¸ç®—è¡¨è¾¾æ¨¡ç³Šä¸æ¸…ï¼Œè¯·æ ¹æ®é—®é¢˜å†…å®¹åˆ¤æ–­æ˜¯å¦éœ€è¦è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
{outlines}
*********
å­¦å‘˜è¾“å…¥:
{user_input}
*********
Analyse the student's input according to the lecture's contents and your criteria.
Output your analysis process step by step.
Finally, output a single letter Y or N in a separate line.
Y means that the input needs to be answered by the teacher.
N means that the input does not needs to be answered by the teacher.""")
```


```python
from langchain_core.output_parsers import BaseOutputParser
import re

class MyOutputParser(BaseOutputParser):
    """è‡ªå®šä¹‰parserï¼Œä»æ€ç»´é“¾ä¸­å–å‡ºæœ€åçš„Y/N"""
    def parse(self, text: str)->str:
        matches = re.findall(r'[YN]', text)
        return matches[-1] if matches else 'N'
```


```python
chain_v2 = (
    need_answer
    | model
    | MyOutputParser()
)
```

å›å½’æµ‹è¯•


```python
run_evaluation(chain_v2, "teacher-assistant", "cot-"+str(uuid.uuid4())[:8])
```

    .............................................................................................

### 1.5ã€Prompt ç‰ˆæœ¬ç®¡ç†

ç›®å‰æ˜¯ beta ç‰ˆæœ¬

<img src="_images/llm/prompt_management.png" width="600px">

ç›®å‰åªæ”¯æŒ Langfuse è‡ªå·±çš„ SDK


```python
# æŒ‰åç§°åŠ è½½
prompt = langfuse.get_prompt("need_answer_v1")
 
# æŒ‰åç§°å’Œç‰ˆæœ¬å·åŠ è½½
prompt = langfuse.get_prompt("need_answer_v1", version=2)
 
# å¯¹æ¨¡æ¿ä¸­çš„å˜é‡èµ‹å€¼
compiled_prompt = prompt.compile(input="è€å¸ˆå¥½",outlines="test")

print(compiled_prompt)
```

    *********
    ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚
     
    è¯¾ç¨‹å†…å®¹:
    test
    *********
    å­¦å‘˜è¾“å…¥:
    è€å¸ˆå¥½
    *********
    å¦‚æœè¿™æ˜¯ä¸€ä¸ªéœ€è¦è€å¸ˆç­”ç–‘çš„é—®é¢˜ï¼Œå›å¤Yï¼Œå¦åˆ™å›å¤Nã€‚
    åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚


### 1.6ã€å¦‚ä½•æ¯”è¾ƒä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼šä¸€äº›ç»å…¸ NLP çš„è¯„æµ‹æ–¹æ³•ï¼ˆé€‰ï¼‰

1. **ç¼–è¾‘è·ç¦»**ï¼šä¹Ÿå«è±æ–‡æ–¯å¦è·ç¦»(Levenshtein),æ˜¯é’ˆå¯¹äºŒä¸ªå­—ç¬¦ä¸²çš„å·®å¼‚ç¨‹åº¦çš„é‡åŒ–é‡æµ‹ï¼Œé‡æµ‹æ–¹å¼æ˜¯çœ‹è‡³å°‘éœ€è¦å¤šå°‘æ¬¡çš„å¤„ç†æ‰èƒ½å°†ä¸€ä¸ªå­—ç¬¦ä¸²å˜æˆå¦ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚
   - å…·ä½“è®¡ç®—è¿‡ç¨‹æ˜¯ä¸€ä¸ªåŠ¨æ€è§„åˆ’ç®—æ³•ï¼šhttps://zhuanlan.zhihu.com/p/164599274
   - è¡¡é‡ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼åº¦æ—¶ï¼Œå¯ä»¥ä»¥è¯ä¸ºå•ä½è®¡ç®—
2. **BLEU Score**:
   - è®¡ç®—è¾“å‡ºä¸å‚ç…§å¥ä¹‹é—´çš„ n-gram å‡†ç¡®ç‡ï¼ˆn=1...4ï¼‰
   - å¯¹çŸ­è¾“å‡ºåšæƒ©ç½š
   - åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Šå¹³å‡ä¸‹è¿°å€¼
   - å®Œæ•´è®¡ç®—å…¬å¼ï¼š$\mathrm{BLEU}_4=\min\left(1,\frac{output-length}{reference-length}\right)\left(\prod_{i=1}^4 precision_i\right)^{\frac{1}{4}}$
   - å‡½æ•°åº“ï¼šhttps://www.nltk.org/_modules/nltk/translate/bleu_score.html
3. **Rouge Score**:
   - Rouge-Nï¼šå°†æ¨¡å‹ç”Ÿæˆçš„ç»“æœå’Œæ ‡å‡†ç»“æœæŒ‰ N-gram æ‹†åˆ†åï¼Œåªè®¡ç®—å¬å›ç‡ï¼›
   - Rouge-L: åˆ©ç”¨äº†æœ€é•¿å…¬å…±å­åºåˆ—ï¼ˆLongest Common Sequenceï¼‰ï¼Œè®¡ç®—ï¼š$P=\frac{LCS(c,r)}{len(c)}$, $R=\frac{LCS(c,r)}{len(r)}$, $F=\frac{(1+\beta^2)PR}{R+\beta^2P}$
   - å‡½æ•°åº“ï¼šhttps://pypi.org/project/rouge-score/
   - å¯¹æ¯” BLEU ä¸ ROUGEï¼š
     - BLEU èƒ½è¯„ä¼°æµç•…åº¦ï¼Œä½†æŒ‡æ ‡åå‘äºè¾ƒçŸ­çš„ç¿»è¯‘ç»“æœï¼ˆbrevity penalty æ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¼ºï¼‰
     - ROUGE ä¸ç®¡æµç•…åº¦ï¼Œæ‰€ä»¥åªé€‚åˆæ·±åº¦å­¦ä¹ çš„ç”Ÿæˆæ¨¡å‹ï¼šç»“æœéƒ½æ˜¯æµç•…çš„å‰æä¸‹ï¼ŒROUGE ååº”å‚ç…§å¥ä¸­å¤šå°‘å†…å®¹è¢«ç”Ÿæˆçš„å¥å­åŒ…å«ï¼ˆå¬å›ï¼‰
5. **METEOR**: å¦ä¸€ä¸ªä»æœºå™¨ç¿»è¯‘é¢†åŸŸå€Ÿé‰´çš„æŒ‡æ ‡ã€‚ä¸ BLEU ç›¸æ¯”ï¼ŒMETEOR è€ƒè™‘äº†æ›´å¤šçš„å› ç´ ï¼Œå¦‚åŒä¹‰è¯åŒ¹é…ã€è¯å¹²åŒ¹é…ã€è¯åºç­‰ï¼Œå› æ­¤å®ƒé€šå¸¸è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªæ›´å…¨é¢çš„è¯„ä»·æŒ‡æ ‡ã€‚
   - å¯¹è¯­è¨€å­¦å’Œè¯­ä¹‰è¯è¡¨æœ‰ä¾èµ–ï¼Œæ‰€ä»¥å¯¹è¯­è¨€ä¾èµ–å¼ºã€‚

<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>æ­¤ç±»æ–¹æ³•å¸¸ç”¨äºå¯¹æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚å®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å…³æ³¨ç›¸å¯¹å˜åŒ–è€Œä¸æ˜¯ç»å¯¹å€¼ï¼ˆè°ƒä¼˜è¿‡ç¨‹ä¸­æŒ‡æ ‡æ˜¯ä¸æ˜¯åœ¨å˜å¥½ï¼‰ã€‚
</div>

### 1.7ã€åŸºäº LLM çš„æµ‹è¯•æ–¹æ³•

LangFuse é›†æˆäº†ä¸€äº›åŸç”Ÿçš„åŸºäº LLM çš„è‡ªåŠ¨æµ‹è¯•æ ‡å‡†ã€‚

å…·ä½“å‚è€ƒï¼šhttps://langfuse.com/docs/scores/model-based-evals

<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>æ­¤ç±»æ–¹æ³•ï¼Œå¯¹äºç”¨äºè¯„ä¼°çš„ LLM è‡ªèº«èƒ½åŠ›æœ‰è¦æ±‚ã€‚éœ€æ ¹æ®å…·ä½“æƒ…å†µé€‰æ‹©ä½¿ç”¨ã€‚
</div>

## 2ã€LangSmith

LangChain å®˜æ–¹çš„ SaaS æœåŠ¡ï¼Œä¸å¼€æºï¼Œæ³¨å†Œéœ€è¦æ’é˜Ÿã€‚

å¹³å°å…¥å£ï¼šhttps://www.langchain.com/langsmith

æ–‡æ¡£åœ°å€ï¼šhttps://python.langchain.com/docs/langsmith/walkthrough

å°†ä½ çš„ LangChain åº”ç”¨ä¸ LangSmith é“¾æ¥ï¼Œéœ€è¦ï¼š

1. æ³¨å†Œè´¦å·ï¼Œå¹¶ç”³è¯·ä¸€ä¸ª`LANGCHAIN_API_KEY`
2. åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®ä»¥ä¸‹å€¼

```shell
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_PROJECT=YOUR_PROJECT_NAME #è‡ªå®šä¹‰é¡¹ç›®åç§°
export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com #LangSmithçš„æœåŠ¡ç«¯ç‚¹
export LANGCHAIN_API_KEY=LANGCHAIN_API_KEY # LangChain API Key
```

3. ç¨‹åºä¸­çš„è°ƒç”¨å°†è‡ªåŠ¨è¢«è®°å½•


```python
import os
os.environ["LANGCHAIN_TRACING_V2"]="true"
os.environ["LANGCHAIN_PROJECT"]="my-test-project"
os.environ["LANGCHAIN_ENDPOINT"]="https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"]="ls__2e0b732f882a45f48e06e26f9862c9da"
```


```python
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

# å®šä¹‰è¯­è¨€æ¨¡å‹
llm = ChatOpenAI()

# å®šä¹‰Promptæ¨¡æ¿
prompt = PromptTemplate.from_template("Say hello to {input}!")

# å®šä¹‰è¾“å‡ºè§£æå™¨
parser = StrOutputParser()

chain = (
    {"input":RunnablePassthrough()} 
    | prompt
    | llm
    | parser
)

chain.invoke("AGIClass")
```




    'Hello AGIClass! How can I assist you today?'



<img src="_images/llm/langsmith-example.png" width="600px">

### 2.1ã€åŸºæœ¬åŠŸèƒ½æ¼”ç¤º

1. Traces
2. LLM Calls
3. Monitor
4. Playground

### 2.2ã€æ•°æ®é›†ç®¡ç†ä¸æµ‹è¯•

### 2.2.1ã€åœ¨çº¿æ ‡æ³¨æ¼”ç¤º

<img src="_images/llm/langsmith-annotation.png" width="600px">

### 2.2.2ã€ä¸Šä¼ æ•°æ®é›†


```python
import json

data = []
with open('my_annotations.jsonl','r',encoding='utf-8') as fp:
    for line in fp:
        example = json.loads(line.strip())
        item = {
            "input": {
                "outlines": example["outlines"],
                "user_input": example["user_input"]
            },
            "expected_output": example["label"]
        }
        data.append(item)
```


```python
from langsmith import Client

client = Client()

dataset_name = "teacher-assistant"

dataset = client.create_dataset(
    dataset_name, #æ•°æ®é›†åç§°
    description="AGIClassçº¿ä¸Šè·Ÿè¯¾åŠ©æ‰‹çš„æ ‡æ³¨æ•°æ®", #æ•°æ®é›†æè¿°
)


client.create_examples(
    inputs=[{"input":item["input"]} for item in data[:50]], 
    outputs=[{"output":item["expected_output"]} for item in data[:50]], 
    dataset_id=dataset.id
)
```

### 2.2.3ã€è¯„ä¼°å‡½æ•°


```python
from langchain.evaluation import StringEvaluator
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import re
from typing import Optional, Any

class AccuracyEvaluator(StringEvaluator):

    def __init__(self):
        pass

    @property
    def requires_input(self) -> bool:
        return False

    @property
    def requires_reference(self) -> bool:
        return True

    @property
    def evaluation_name(self) -> str:
        return "accuracy"

    def _evaluate_strings(
        self,
        prediction: str,
        input: Optional[str] = None,
        reference: Optional[str] = None,
        **kwargs: Any
    ) -> dict:
        return {"score": int(prediction==reference)}
```


```python
from langchain.evaluation import EvaluatorType
from langchain.smith import RunEvalConfig

evaluation_config = RunEvalConfig(
    # è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†
    custom_evaluators=[AccuracyEvaluator()],
)
```

### 2.2.4ã€è¿è¡Œæµ‹è¯•


```python
from langchain.prompts import PromptTemplate

need_answer=PromptTemplate.from_template("""
*********
ä½ æ˜¯AIGCè¯¾ç¨‹çš„åŠ©æ•™ï¼Œä½ çš„å·¥ä½œæ˜¯ä»å­¦å‘˜çš„è¯¾å ‚äº¤æµä¸­é€‰æ‹©å‡ºéœ€è¦è€å¸ˆå›ç­”çš„é—®é¢˜ï¼ŒåŠ ä»¥æ•´ç†ä»¥äº¤ç»™è€å¸ˆå›ç­”ã€‚
 
è¯¾ç¨‹å†…å®¹:
{outlines}
*********
å­¦å‘˜è¾“å…¥:
{user_input}
*********
å¦‚æœè¿™æ˜¯ä¸€ä¸ªéœ€è¦è€å¸ˆç­”ç–‘çš„é—®é¢˜ï¼Œå›å¤Yï¼Œå¦åˆ™å›å¤Nã€‚
åªå›å¤Yæˆ–Nï¼Œä¸è¦å›å¤å…¶ä»–å†…å®¹ã€‚""")

model = ChatOpenAI(temperature=0,model_kwargs={"seed":42})
parser = StrOutputParser()

chain_v1 = (
    {
        "outlines":lambda x: x["input"]["outlines"],
        "user_input":lambda x: x["input"]["user_input"],
    }
    | need_answer
    | model
    | parser
)
```


```python
from langchain.smith import (
    arun_on_dataset,
    run_on_dataset,
)

results = await arun_on_dataset(
    dataset_name=dataset_name,
    llm_or_chain_factory=chain_v1,
    evaluation=evaluation_config,
    verbose=True,
    client=client,
    project_name="test-my-assistant-v1",
    tags=[
        "prompt_v1",
    ],  # å¯é€‰ï¼Œè‡ªå®šä¹‰çš„æ ‡è¯†
)
```

    View the evaluation results for project 'test-my-assistant-v1' at:
    https://smith.langchain.com/o/97b8262a-9ab9-4b43-afeb-21ea05a90ba7/datasets/b8f34ad2-126b-47ee-a00b-80b9615f318a/compare?selectedSessions=e1742fe4-9c53-422d-a0a6-381d9d9753e4
    
    View all tests for Dataset teacher-assistant at:
    https://smith.langchain.com/o/97b8262a-9ab9-4b43-afeb-21ea05a90ba7/datasets/b8f34ad2-126b-47ee-a00b-80b9615f318a
    [------------------------------------------------->] 50/50


<h3>Experiment Results:</h3>



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>output</th>
      <th>feedback.accuracy</th>
      <th>error</th>
      <th>execution_time</th>
      <th>run_id</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>50</td>
      <td>50.000000</td>
      <td>0</td>
      <td>50.000000</td>
      <td>50</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>2</td>
      <td>NaN</td>
      <td>0</td>
      <td>NaN</td>
      <td>50</td>
    </tr>
    <tr>
      <th>top</th>
      <td>N</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>ef33817b-1118-474b-ab31-8984b3cfadf1</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>31</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>1</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>NaN</td>
      <td>0.780000</td>
      <td>NaN</td>
      <td>1.352174</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>std</th>
      <td>NaN</td>
      <td>0.418452</td>
      <td>NaN</td>
      <td>0.353752</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>min</th>
      <td>NaN</td>
      <td>0.000000</td>
      <td>NaN</td>
      <td>1.006048</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>NaN</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>1.240261</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>NaN</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>1.289699</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>NaN</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>1.348476</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>max</th>
      <td>NaN</td>
      <td>1.000000</td>
      <td>NaN</td>
      <td>3.432141</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>


### 2.2.5ã€åŸºäº LLM çš„è¯„ä¼°å‡½æ•°

https://docs.smith.langchain.com/evaluation/evaluator-implementations

## 3ã€Prompt Flow

<img src="_images/llm/prompt-flow.png" width="600px">

é¡¹ç›®åœ°å€ https://github.com/microsoft/promptflow

### 3.1ã€å®‰è£…

```sh
pip install promptflow promptflow-tools
```

### 3.2ã€å‘½ä»¤è¡Œè¿è¡Œ

```sh
pf flow init --flow ./my_chatbot --type chat
```

### 3.3ã€VSCode æ’ä»¶

https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow

<img src="_images/llm/vsc.png" width="600px">

### 3.4ã€ä¸ Semantic Kernel ç»“åˆä½¿ç”¨

<æ¼”ç¤º>

Azureäº‘æœåŠ¡ï¼šhttps://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/get-started-prompt-flow?view=azureml-api-2

## æ€»ç»“

ç®¡ç†ä¸€ä¸ª LLM åº”ç”¨çš„å…¨ç”Ÿå‘½å‘¨æœŸï¼Œéœ€è¦ç”¨åˆ°ä»¥ä¸‹å·¥å…·ï¼š

1. è°ƒè¯• Prompt çš„ Playground
2. æµ‹è¯•/éªŒè¯ç³»ç»Ÿçš„ç›¸å…³æŒ‡æ ‡
3. æ•°æ®é›†ç®¡ç†
4. å„ç§æŒ‡æ ‡ç›‘æ§ä¸ç»Ÿè®¡ï¼šè®¿é—®é‡ã€å“åº”æ—¶é•¿ã€Tokenè´¹ç­‰ç­‰

æ ¹æ®è‡ªå·±çš„æŠ€æœ¯æ ˆï¼Œé€‰æ‹©ï¼š

1. LangFuseï¼šå¼€æºå¹³å°ï¼Œæ”¯æŒ LangChain å’ŒåŸç”Ÿ OpenAI API
2. LangSmith: LangChain çš„åŸå§‹ç®¡ç†å¹³å°
3. Prompt Flowï¼šå¼€æºå¹³å°ï¼Œæ”¯æŒ Semantic Kernel

## ä½œä¸š

é€‰æ‹©ä¸€ä¸ªå·¥å…·å¹³å°ï¼Œå¯¹è‡ªå·±ä¹‹å‰å¼€å‘çš„ç³»ç»Ÿæˆ–æ¨¡å‹åšæ‰¹é‡æµ‹è¯•
