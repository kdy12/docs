## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ 


1. å¦‚ä½•ç”¨ä½ çš„å‚åŸŸæ•°æ®è¡¥å…… LLM çš„èƒ½åŠ›
1. å¦‚ä½•æ„å»ºä½ çš„å‚åŸŸï¼ˆå‘é‡ï¼‰çŸ¥è¯†åº“
1. æ­å»ºä¸€å¥—å®Œæ•´ RAG ç³»ç»Ÿéœ€è¦å“ªäº›æ¨¡å—

å¼€å§‹ä¸Šè¯¾ï¼


## ä¸€ã€æ¾„æ¸…ä¸€ä¸ªæ¦‚å¿µ

RAG **ä¸è¦** å‚è€ƒä¸‹é¢è¿™å¼ å›¾ï¼ï¼ï¼

<img src="_images/llm/rag-paper.png" style="margin-left: 0px" width="600px">

è¿™å¼ å›¾æºè‡ªä¸€ä¸ª[ç ”ç©¶å·¥ä½œ](https://arxiv.org/pdf/2005.11401.pdf)
- æ­¤è®ºæ–‡ç¬¬ä¸€æ¬¡æå‡º RAG è¿™ä¸ªå«æ³•
- åœ¨ç ”ç©¶ä¸­ï¼Œä½œè€…å°è¯•å°†æ£€ç´¢å’Œç”Ÿæˆåšåœ¨ä¸€ä¸ªæ¨¡å‹ä½“ç³»ä¸­

**ä½†æ˜¯ï¼Œå®é™…ç”Ÿäº§ä¸­ï¼ŒRAG ä¸æ˜¯è¿™ä¹ˆåšçš„ï¼ï¼ï¼**

## äºŒã€ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºçš„ç”Ÿæˆæ¨¡å‹ï¼ˆRAGï¼‰


### 2.1ã€LLM å›ºæœ‰çš„å±€é™æ€§

1. LLM çš„çŸ¥è¯†ä¸æ˜¯å®æ—¶çš„
2. LLM å¯èƒ½ä¸çŸ¥é“ä½ ç§æœ‰çš„é¢†åŸŸ/ä¸šåŠ¡çŸ¥è¯†

<img src="_images/llm/gpt-llama2.png" style="margin-left: 0px" width="600px">


### 2.2ã€æ£€ç´¢å¢å¼ºç”Ÿæˆ


RAGï¼ˆRetrieval Augmented Generationï¼‰é¡¾åæ€ä¹‰ï¼Œé€šè¿‡**æ£€ç´¢**çš„æ–¹æ³•æ¥å¢å¼º**ç”Ÿæˆæ¨¡å‹**çš„èƒ½åŠ›ã€‚

<video src="RAG.mp4" controls="controls" width=800px style="margin-left: 0px"></video>


<div class="alert alert-success">
<b>ç±»æ¯”ï¼š</b>ä½ å¯ä»¥æŠŠè¿™ä¸ªè¿‡ç¨‹æƒ³è±¡æˆå¼€å·è€ƒè¯•ã€‚è®© LLM å…ˆç¿»ä¹¦ï¼Œå†å›ç­”é—®é¢˜ã€‚
</div>


## ä¸‰ã€RAG ç³»ç»Ÿçš„åŸºæœ¬æ­å»ºæµç¨‹


å…ˆçœ‹æ•ˆæœï¼šhttp://localhost:9999/

æ­å»ºè¿‡ç¨‹ï¼š

1. æ–‡æ¡£åŠ è½½ï¼Œå¹¶æŒ‰ä¸€å®šæ¡ä»¶**åˆ‡å‰²**æˆç‰‡æ®µ
2. å°†åˆ‡å‰²çš„æ–‡æœ¬ç‰‡æ®µçŒå…¥**æ£€ç´¢å¼•æ“**
3. å°è£…**æ£€ç´¢æ¥å£**
4. æ„å»º**è°ƒç”¨æµç¨‹**ï¼šQuery -> æ£€ç´¢ -> Prompt -> LLM -> å›å¤


### 3.1ã€æ–‡æ¡£çš„åŠ è½½ä¸åˆ‡å‰²



```python
# å®‰è£… pdf è§£æåº“
!pip install pdfminer.six
```


```python
from pdfminer.high_level import extract_pages
from pdfminer.layout import LTTextContainer
```


```python
def extract_text_from_pdf(filename, page_numbers=None, min_line_length=1):
    '''ä» PDF æ–‡ä»¶ä¸­ï¼ˆæŒ‰æŒ‡å®šé¡µç ï¼‰æå–æ–‡å­—'''
    paragraphs = []
    buffer = ''
    full_text = ''
    # æå–å…¨éƒ¨æ–‡æœ¬
    for i, page_layout in enumerate(extract_pages(filename)):
        # å¦‚æœæŒ‡å®šäº†é¡µç èŒƒå›´ï¼Œè·³è¿‡èŒƒå›´å¤–çš„é¡µ
        if page_numbers is not None and i not in page_numbers:
            continue
        for element in page_layout:
            if isinstance(element, LTTextContainer):
                full_text += element.get_text() + '\n'
    # æŒ‰ç©ºè¡Œåˆ†éš”ï¼Œå°†æ–‡æœ¬é‡æ–°ç»„ç»‡æˆæ®µè½
    lines = full_text.split('\n')
    for text in lines:
        if len(text) >= min_line_length:
            buffer += (' '+text) if not text.endswith('-') else text.strip('-')
        elif buffer:
            paragraphs.append(buffer)
            buffer = ''
    if buffer:
        paragraphs.append(buffer)
    return paragraphs
```


```python
paragraphs = extract_text_from_pdf("llama2.pdf", min_line_length=10)
```


```python
for para in paragraphs[:3]:
    print(para+"\n")
```

     Llama 2: Open Foundation and Fine-Tuned Chat Models
    
     Hugo Touvronâˆ— Louis Martinâ€  Kevin Stoneâ€  Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialomâˆ—
    
     GenAI, Meta


â€‹    

### 3.2ã€æ£€ç´¢å¼•æ“


å…ˆçœ‹ä¸€ä¸ªæœ€åŸºç¡€çš„å®ç°



```python
# å®‰è£… ES å®¢æˆ·ç«¯
!pip install elasticsearch7
# å®‰è£…NLTKï¼ˆæ–‡æœ¬å¤„ç†æ–¹æ³•åº“ï¼‰
!pip install nltk
```


```python
from elasticsearch7 import Elasticsearch, helpers
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import re

import warnings
warnings.simplefilter("ignore")  # å±è”½ ES çš„ä¸€äº›Warnings

nltk.download('punkt')  # è‹±æ–‡åˆ‡è¯ã€è¯æ ¹ã€åˆ‡å¥ç­‰æ–¹æ³•
nltk.download('stopwords')  # è‹±æ–‡åœç”¨è¯åº“
```

    [nltk_data] Downloading package punkt to /home/jovyan/nltk_data...
    [nltk_data]   Package punkt is already up-to-date!
    [nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!





    True




```python
def to_keywords(input_string):
    '''ï¼ˆè‹±æ–‡ï¼‰æ–‡æœ¬åªä¿ç•™å…³é”®å­—'''
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰€æœ‰éå­—æ¯æ•°å­—çš„å­—ç¬¦ä¸ºç©ºæ ¼
    no_symbols = re.sub(r'[^a-zA-Z0-9\s]', ' ', input_string)
    word_tokens = word_tokenize(no_symbols)
    # åŠ è½½åœç”¨è¯è¡¨
    stop_words = set(stopwords.words('english'))
    ps = PorterStemmer()
    # å»åœç”¨è¯ï¼Œå–è¯æ ¹
    filtered_sentence = [ps.stem(w)
                         for w in word_tokens if not w.lower() in stop_words]
    return ' '.join(filtered_sentence)
```

<div class="alert alert-info">
æ­¤å¤„ to_keywords ä¸ºé’ˆå¯¹è‹±æ–‡çš„å®ç°ï¼Œé’ˆå¯¹ä¸­æ–‡çš„å®ç°è¯·å‚è€ƒ chinese_utils.py
</div>

å°†æ–‡æœ¬çŒå…¥æ£€ç´¢å¼•æ“



```python
# 1. åˆ›å»ºElasticsearchè¿æ¥
es = Elasticsearch(
    hosts=['http://117.50.198.53:9200'],  # æœåŠ¡åœ°å€ä¸ç«¯å£
    http_auth=("elastic", "FKaB1Jpz0Rlw0l6G"),  # ç”¨æˆ·åï¼Œå¯†ç 
)

# 2. å®šä¹‰ç´¢å¼•åç§°
index_name = "teacher_demo_index123"

# 3. å¦‚æœç´¢å¼•å·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒï¼ˆä»…ä¾›æ¼”ç¤ºï¼Œå®é™…åº”ç”¨æ—¶ä¸éœ€è¦è¿™æ­¥ï¼‰
if es.indices.exists(index=index_name):
    es.indices.delete(index=index_name)

# 4. åˆ›å»ºç´¢å¼•
es.indices.create(index=index_name)

# 5. çŒåº“æŒ‡ä»¤
actions = [
    {
        "_index": index_name,
        "_source": {
            "keywords": to_keywords(para),
            "text": para
        }
    }
    for para in paragraphs
]

# 6. æ–‡æœ¬çŒåº“
helpers.bulk(es, actions)
```




    (983, [])



å®ç°å…³é”®å­—æ£€ç´¢



```python
def search(query_string, top_n=3):
    # ES çš„æŸ¥è¯¢è¯­è¨€
    search_query = {
        "match": {
            "keywords": to_keywords(query_string)
        }
    }
    res = es.search(index=index_name, query=search_query, size=top_n)
    return [hit["_source"]["text"] for hit in res["hits"]["hits"]]
```


```python
results = search("how many parameters does llama 2 have?", 2)
for r in results:
    print(r+"\n")
```

     Llama 2 comes in a range of parameter sizesâ€”7B, 13B, and 70Bâ€”as well as pretrained and fine-tuned variations.
    
     1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§


â€‹    

### 3.3ã€LLM æ¥å£å°è£…



```python
from openai import OpenAI
import os
# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())  # è¯»å–æœ¬åœ° .env æ–‡ä»¶ï¼Œé‡Œé¢å®šä¹‰äº† OPENAI_API_KEY

client = OpenAI()
```


```python
def get_completion(prompt, model="gpt-3.5-turbo"):
    '''å°è£… openai æ¥å£'''
    messages = [{"role": "user", "content": prompt}]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0,  # æ¨¡å‹è¾“å‡ºçš„éšæœºæ€§ï¼Œ0 è¡¨ç¤ºéšæœºæ€§æœ€å°
    )
    return response.choices[0].message.content
```

### 3.4ã€Prompt æ¨¡æ¿



```python
def build_prompt(prompt_template, **kwargs):
    '''å°† Prompt æ¨¡æ¿èµ‹å€¼'''
    prompt = prompt_template
    for k, v in kwargs.items():
        if isinstance(v, str):
            val = v
        elif isinstance(v, list) and all(isinstance(elem, str) for elem in v):
            val = '\n'.join(v)
        else:
            val = str(v)
        prompt = prompt.replace(f"__{k.upper()}__", val)
    return prompt
```


```python
prompt_template = """
ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
ç¡®ä¿ä½ çš„å›å¤å®Œå…¨ä¾æ®ä¸‹è¿°å·²çŸ¥ä¿¡æ¯ã€‚ä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
å¦‚æœä¸‹è¿°å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤"æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜"ã€‚

å·²çŸ¥ä¿¡æ¯:
__INFO__

ç”¨æˆ·é—®ï¼š
__QUERY__

è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
"""
```

### 3.5ã€RAG Pipeline åˆæ¢



```python
user_query = "how many parameters does llama 2 have?"

# 1. æ£€ç´¢
search_results = search(user_query, 2)

# 2. æ„å»º Prompt
prompt = build_prompt(prompt_template, info=search_results, query=user_query)
print("===Prompt===")
print(prompt)

# 3. è°ƒç”¨ LLM
response = get_completion(prompt)

print("===å›å¤===")
print(response)
```

    ===Prompt===
    
    ä½ æ˜¯ä¸€ä¸ªé—®ç­”æœºå™¨äººã€‚
    ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸‹è¿°ç»™å®šçš„å·²çŸ¥ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    ç¡®ä¿ä½ çš„å›å¤å®Œå…¨ä¾æ®ä¸‹è¿°å·²çŸ¥ä¿¡æ¯ã€‚ä¸è¦ç¼–é€ ç­”æ¡ˆã€‚
    å¦‚æœä¸‹è¿°å·²çŸ¥ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·ç›´æ¥å›å¤"æˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜"ã€‚
    
    å·²çŸ¥ä¿¡æ¯:
     Llama 2 comes in a range of parameter sizesâ€”7B, 13B, and 70Bâ€”as well as pretrained and fine-tuned variations.
     1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§
    
    ç”¨æˆ·é—®ï¼š
    how many parameters does llama 2 have?
    
    è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    
    ===å›å¤===
    Llama 2æœ‰7Bã€13Bå’Œ70Bä¸‰ç§å‚æ•°å¤§å°çš„å˜ä½“ã€‚


<div class="alert alert-info">
<b>æ‰©å±•é˜…è¯»ï¼š</b>
<ol>
<ul>Elasticsearchï¼ˆç®€ç§°ESï¼‰æ˜¯ä¸€ä¸ªå¹¿æ³›åº”ç”¨çš„å¼€æºæœç´¢å¼•æ“: https://www.elastic.co/</ul>
<ul>å…³äºESçš„å®‰è£…ã€éƒ¨ç½²ç­‰çŸ¥è¯†ï¼Œç½‘ä¸Šå¯ä»¥æ‰¾åˆ°å¤§é‡èµ„æ–™ï¼Œä¾‹å¦‚: https://juejin.cn/post/7104875268166123528</ul>
<ul>å…³äºç»å…¸ä¿¡æ¯æ£€ç´¢æŠ€æœ¯çš„æ›´å¤šç»†èŠ‚ï¼Œå¯ä»¥å‚è€ƒ: https://nlp.stanford.edu/IR-book/information-retrieval-book.html</ul>
</div>


### 3.6ã€å…³é”®å­—æ£€ç´¢çš„å±€é™æ€§


åŒä¸€ä¸ªè¯­ä¹‰ï¼Œç”¨è¯ä¸åŒï¼Œå¯èƒ½å¯¼è‡´æ£€ç´¢ä¸åˆ°æœ‰æ•ˆçš„ç»“æœ



```python
# user_query="Does llama 2 have a chat version?"
user_query = "Does llama 2 have a conversational variant?"

search_results = search(user_query, 2)

for res in search_results:
    print(res+"\n")
```

     1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§
    
     In Figure 18, we report the violation percentage on single- and multi-turn conversations, respectively. A trend across models is that multi-turn conversations are more prone to inducing unsafe responses. That said, Llama 2-Chat still performs well compared to baselines, especially on multi-turn conversations. We also observe that Falcon performs particularly well on single-turn conversations (largely due to its conciseness) but much worse on multi-turn conversations, which could be due to its lack of multi-turn supervised fine-tuning data.


â€‹    

## å››ã€å‘é‡æ£€ç´¢


### 4.1ã€æ–‡æœ¬å‘é‡ï¼ˆText Embeddingsï¼‰


1. å°†æ–‡æœ¬è½¬æˆä¸€ç»„æµ®ç‚¹æ•°ï¼šæ¯ä¸ªä¸‹æ ‡ $i$ï¼Œå¯¹åº”ä¸€ä¸ªç»´åº¦
2. æ•´ä¸ªæ•°ç»„å¯¹åº”ä¸€ä¸ª $n$ ç»´ç©ºé—´çš„ä¸€ä¸ªç‚¹ï¼Œå³**æ–‡æœ¬å‘é‡**åˆå« Embeddings
3. å‘é‡ä¹‹é—´å¯ä»¥è®¡ç®—è·ç¦»ï¼Œè·ç¦»è¿œè¿‘å¯¹åº”**è¯­ä¹‰ç›¸ä¼¼åº¦**å¤§å°

<br />
<img src="_images/llm/embeddings.png" style="margin-left: 0px" width=800px>
<br />


### 4.1.1ã€æ–‡æœ¬å‘é‡æ˜¯æ€ä¹ˆå¾—åˆ°çš„ï¼ˆé€‰ï¼‰

1. æ„å»ºç›¸å…³ï¼ˆæ­£ç«‹ï¼‰ä¸ä¸ç›¸å…³ï¼ˆè´Ÿä¾‹ï¼‰çš„å¥å­å¯¹å„¿æ ·æœ¬
2. è®­ç»ƒåŒå¡”å¼æ¨¡å‹ï¼Œè®©æ­£ä¾‹é—´çš„è·ç¦»å°ï¼Œè´Ÿä¾‹é—´çš„è·ç¦»å¤§

ä¾‹å¦‚ï¼š

<img src="_images/llm/sbert.png" style="margin-left: 0px" width=500px>


<div class="alert alert-info">
<b>æ‰©å±•é˜…è¯»ï¼šhttps://www.sbert.net</b>
</div>


### 4.2ã€å‘é‡é—´çš„ç›¸ä¼¼åº¦è®¡ç®—

<img src="_images/llm/sim.png" style="margin-left: 0px" width=500px>



```python
import numpy as np
from numpy import dot
from numpy.linalg import norm
```


```python
def cos_sim(a, b):
    '''ä½™å¼¦è·ç¦» -- è¶Šå¤§è¶Šç›¸ä¼¼'''
    return dot(a, b)/(norm(a)*norm(b))


def l2(a, b):
    '''æ¬§å¼è·ç¦» -- è¶Šå°è¶Šç›¸ä¼¼'''
    x = np.asarray(a)-np.asarray(b)
    return norm(x)
```


```python
def get_embeddings(texts, model="text-embedding-ada-002"):
    '''å°è£… OpenAI çš„ Embedding æ¨¡å‹æ¥å£'''
    data = client.embeddings.create(input=texts, model=model).data
    return [x.embedding for x in data]
```


```python
test_query = ["æµ‹è¯•æ–‡æœ¬"]
vec = get_embeddings(test_query)[0]
print(vec[:10])
print(len(vec))
```

    [-0.0072620222344994545, -0.006227712146937847, -0.010517913848161697, 0.001511403825134039, -0.010678159072995186, 0.029252037405967712, -0.019783001393079758, 0.0053937085904181, -0.017029697075486183, -0.01215678546577692]
    1536



```python
# query = "å›½é™…äº‰ç«¯"

# ä¸”èƒ½æ”¯æŒè·¨è¯­è¨€
query = "global conflicts"

documents = [
    "è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œå°”åœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š",
    "åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤",
    "æ—¥æœ¬å²é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤",
    "å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥",
    "æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ",
]

query_vec = get_embeddings([query])[0]
doc_vecs = get_embeddings(documents)

print("Cosine distance:")
print(cos_sim(query_vec, query_vec))
for vec in doc_vecs:
    print(cos_sim(query_vec, vec))

print("\nEuclidean distance:")
print(l2(query_vec, query_vec))
for vec in doc_vecs:
    print(l2(query_vec, vec))
```

    Cosine distance:
    1.0
    0.7631678388619634
    0.757462568438369
    0.7438386573552248
    0.7090619887723215
    0.7265344657437772
    
    Euclidean distance:
    0.0
    0.6882327760049508
    0.6964731624559146
    0.7157672101611734
    0.7628080038023212
    0.7395478750135311


### 4.3ã€å‘é‡æ•°æ®åº“


å‘é‡æ•°æ®åº“ï¼Œæ˜¯ä¸“é—¨ä¸ºå‘é‡æ£€ç´¢è®¾è®¡çš„ä¸­é—´ä»¶



```python
!pip install chromadb
```


```python
# ä¸ºäº†æ¼”ç¤ºæ–¹ä¾¿ï¼Œæˆ‘ä»¬åªå–ä¸¤é¡µï¼ˆç¬¬ä¸€ç« ï¼‰
paragraphs = extract_text_from_pdf("llama2.pdf", page_numbers=[
                                   2, 3], min_line_length=10)
```


```python
import chromadb
from chromadb.config import Settings


class MyVectorDBConnector:
    def __init__(self, collection_name, embedding_fn):
        chroma_client = chromadb.Client(Settings(allow_reset=True))

        # ä¸ºäº†æ¼”ç¤ºï¼Œå®é™…ä¸éœ€è¦æ¯æ¬¡ reset()
        chroma_client.reset()

        # åˆ›å»ºä¸€ä¸ª collection
        self.collection = chroma_client.get_or_create_collection(name=collection_name)
        self.embedding_fn = embedding_fn

    def add_documents(self, documents):
        '''å‘ collection ä¸­æ·»åŠ æ–‡æ¡£ä¸å‘é‡'''
        self.collection.add(
            embeddings=self.embedding_fn(documents),  # æ¯ä¸ªæ–‡æ¡£çš„å‘é‡
            documents=documents,  # æ–‡æ¡£çš„åŸæ–‡
            ids=[f"id{i}" for i in range(len(documents))]  # æ¯ä¸ªæ–‡æ¡£çš„ id
        )

    def search(self, query, top_n):
        '''æ£€ç´¢å‘é‡æ•°æ®åº“'''
        results = self.collection.query(
            query_embeddings=self.embedding_fn([query]),
            n_results=top_n
        )
        return results
```


```python
# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡
vector_db = MyVectorDBConnector("demo", get_embeddings)
# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£
vector_db.add_documents(paragraphs)
```


```python
user_query = "Llama 2æœ‰å¤šå°‘å‚æ•°"
results = vector_db.search(user_query, 2)
```


```python
for para in results['documents'][0]:
    print(para+"\n")
```

     1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§
    
     In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.


â€‹    

### 4.3.1ã€å‘é‡æ•°æ®åº“æœåŠ¡


Server ç«¯

```sh
chroma run --path /db_path
```

Client ç«¯

```python
import chromadb
chroma_client = chromadb.HttpClient(host='localhost', port=8000)
```


### 4.3.2ã€ä¸»æµå‘é‡æ•°æ®åº“åŠŸèƒ½å¯¹æ¯”

<img src="_images/llm/vectordb.png" style="margin-left: 0px" width=800px>


- FAISS: Meta å¼€æºçš„å‘é‡æ£€ç´¢å¼•æ“ https://github.com/facebookresearch/faiss
- Pinecone: å•†ç”¨å‘é‡æ•°æ®åº“ï¼Œåªæœ‰äº‘æœåŠ¡ https://www.pinecone.io/
- Milvus: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://milvus.io/
- Weaviate: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://weaviate.io/
- Qdrant: å¼€æºå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶æœ‰äº‘æœåŠ¡ https://qdrant.tech/
- PGVector: Postgres çš„å¼€æºå‘é‡æ£€ç´¢å¼•æ“ https://github.com/pgvector/pgvector
- RediSearch: Redis çš„å¼€æºå‘é‡æ£€ç´¢å¼•æ“ https://github.com/RediSearch/RediSearch
- ElasticSearch ä¹Ÿæ”¯æŒå‘é‡æ£€ç´¢ https://www.elastic.co/enterprise-search/vector-search


### 4.4ã€åŸºäºå‘é‡æ£€ç´¢çš„ RAG



```python
class RAG_Bot:
    def __init__(self, vector_db, llm_api, n_results=2):
        self.vector_db = vector_db
        self.llm_api = llm_api
        self.n_results = n_results

    def chat(self, user_query):
        # 1. æ£€ç´¢
        search_results = self.vector_db.search(user_query, self.n_results)

        # 2. æ„å»º Prompt
        prompt = build_prompt(
            prompt_template, info=search_results['documents'][0], query=user_query)

        # 3. è°ƒç”¨ LLM
        response = self.llm_api(prompt)
        return response
```


```python
# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº
bot = RAG_Bot(
    vector_db,
    llm_api=get_completion
)

user_query = "llama 2æœ‰å¯¹è¯ç‰ˆå—ï¼Ÿ"

response = bot.chat(user_query)

print(response)
```

    æ˜¯çš„ï¼ŒLlama 2æœ‰å¯¹è¯ç‰ˆï¼Œå®ƒè¢«ç§°ä¸ºLlama 2-Chatï¼Œæ˜¯ç»è¿‡ä¼˜åŒ–ç”¨äºå¯¹è¯åœºæ™¯çš„ç‰ˆæœ¬ã€‚


### 4.5ã€å¦‚æœæƒ³è¦æ¢ä¸ªæ¨¡å‹



```python
import json
import requests
import os

# é€šè¿‡é‰´æƒæ¥å£è·å– access token
def get_access_token():
    """
    ä½¿ç”¨ AKï¼ŒSK ç”Ÿæˆé‰´æƒç­¾åï¼ˆAccess Tokenï¼‰
    :return: access_tokenï¼Œæˆ–æ˜¯None(å¦‚æœé”™è¯¯)
    """
    url = "https://aip.baidubce.com/oauth/2.0/token"
    params = {
        "grant_type": "client_credentials",
        "client_id": os.getenv('ERNIE_CLIENT_ID'),
        "client_secret": os.getenv('ERNIE_CLIENT_SECRET')
    }

    return str(requests.post(url, params=params).json().get("access_token"))

# è°ƒç”¨æ–‡å¿ƒåƒå¸† è°ƒç”¨ BGE Embedding æ¥å£
def get_embeddings_bge(prompts):
    url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/embeddings/bge_large_en?access_token=" + get_access_token()
    payload = json.dumps({
        "input": prompts
    })
    headers = {'Content-Type': 'application/json'}

    response = requests.request(
        "POST", url, headers=headers, data=payload).json()
    data = response["data"]
    return [x["embedding"] for x in data]


# è°ƒç”¨æ–‡å¿ƒ4.0å¯¹è¯æ¥å£
def get_completion_ernie(prompt):

    url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions_pro?access_token=" + get_access_token()
    payload = json.dumps({
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ]
    })

    headers = {'Content-Type': 'application/json'}

    response = requests.request(
        "POST", url, headers=headers, data=payload).json()

    return response["result"]
```


```python
# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡
new_vector_db = MyVectorDBConnector(
    "demo_ernie",
    embedding_fn=get_embeddings_bge
)
# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£
new_vector_db.add_documents(paragraphs)

# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº
new_bot = RAG_Bot(
    new_vector_db,
    llm_api=get_completion_ernie
)
```


```python
user_query = "how many parameters does llama 2 have?"

response = new_bot.chat(user_query)

print(response)
```

    Llama 2æœ‰7Bã€13Bå’Œ70Bä¸‰ç§å‚æ•°çš„ç‰ˆæœ¬ã€‚


## äº”ã€å®æˆ˜ RAG ç³»ç»Ÿçš„è¿›é˜¶çŸ¥è¯†


### 5.1ã€æ–‡æœ¬åˆ†å‰²çš„ç²’åº¦


**ç¼ºé™·**

1. ç²’åº¦å¤ªå¤§å¯èƒ½å¯¼è‡´æ£€ç´¢ä¸ç²¾å‡†ï¼Œç²’åº¦å¤ªå°å¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸å…¨é¢
2. é—®é¢˜çš„ç­”æ¡ˆå¯èƒ½è·¨è¶Šä¸¤ä¸ªç‰‡æ®µ



```python
# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡
vector_db = MyVectorDBConnector("demo_text_split", get_embeddings)
# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£
vector_db.add_documents(paragraphs)

# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº
bot = RAG_Bot(
    vector_db,
    llm_api=get_completion
)
```


```python
# user_query = "llama 2å¯ä»¥å•†ç”¨å—ï¼Ÿ"
user_query="llama 2 chatæœ‰å¤šå°‘å‚æ•°"
search_results = vector_db.search(user_query, 2)

for doc in search_results['documents'][0]:
    print(doc+"\n")

print("====å›å¤====")
bot.chat(user_query)
```

     In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.
    
     2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release
    
    ====å›å¤====





    'Llama 2-Chatæ¨¡å‹çš„å‚æ•°æ•°é‡æ˜¯70Bã€‚'




```python
for p in paragraphs:
    print(p+"\n")
```

     Figure 1: Helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of both single and multi-turn prompts. The 95% confidence intervals for this evaluation are between 1% and 2%. More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent difficulty of comparing generations.
    
     Figure 2: Win-rate % for helpfulness andsafety between commercial-licensed baselines and Llama 2-Chat, according to GPT 4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias.
    
     1 Introduction
    
     Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized domains such as programming and creative writing. They enable interaction with humans through intuitive chat interfaces, which has led to rapid and widespread adoption among the general public.
    
     The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data, followed by alignment with human preferences via techniques such as Reinforcement Learning with Human Feedback (RLHF). Although the training methodology is simple, high computational requirements have limited the development of LLMs to a few players. There have been public releases of pretrained LLMs (such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla (Hoffmann et al., 2022), but none of these models are suitable substitutes for closed â€œproductâ€ LLMs, such as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human preferences, which greatly enhances their usability and safety. This step can require significant costs in compute and human annotation, and is often not transparent or easily reproducible, limiting progress within the community to advance AI alignment research.
    
     In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge.
    
    Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models. Human raters judged model generations for safety violations across ~2,000 adversarial prompts consisting of both single and multi-turn prompts. More details can be found in Section 4.4. It is important to caveat these safety results with the inherent bias of LLM evaluations due to limitations of the prompt set, subjectivity of the review guidelines, and subjectivity of individual raters. Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models.
    
     We are releasing the following models to the general public for research and commercial useâ€¡:
    
     1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data. We also increased the size of the pretraining corpus by 40%, doubled the context length of the model, and adopted grouped-query attention (Ainslie et al., 2023). We are releasing variants of Llama 2 with 7B, 13B, and 70B parameters. We have also trained 34B variants, which we report on in this paper but are not releasing.Â§
    
     2. Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release
    
     variants of this model with 7B, 13B, and 70B parameters as well.
    
     We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023). Testing conducted to date has been in English and has not â€” and could not â€” cover all scenarios. Therefore, before deploying any applications of Llama 2-Chat, developers should perform safety testing and tuning tailored to their specific applications of the model. We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.
    
     The remainder of this paper describes our pretraining methodology (Section 2), fine-tuning methodology (Section 3), approach to model safety (Section 4), key observations and insights (Section 5), relevant related work (Section 6), and conclusions (Section 7).
    
     â€¡https://ai.meta.com/resources/models-and-libraries/llama/ Â§We are delaying the release of the 34B model due to a lack of time to sufficiently red team. Â¶https://ai.meta.com/llama â€–https://github.com/facebookresearch/llama


â€‹    

**æ”¹è¿›**: æŒ‰ä¸€å®šç²’åº¦ï¼Œéƒ¨åˆ†é‡å å¼çš„åˆ‡å‰²æ–‡æœ¬ï¼Œä½¿ä¸Šä¸‹æ–‡æ›´å®Œæ•´



```python
from nltk.tokenize import sent_tokenize
import json


def split_text(paragraphs, chunk_size=300, overlap_size=100):
    '''æŒ‰æŒ‡å®š chunk_size å’Œ overlap_size äº¤å å‰²æ–‡æœ¬'''
    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]
    chunks = []
    i = 0
    while i < len(sentences):
        chunk = sentences[i]
        overlap = ''
        prev_len = 0
        prev = i - 1
        # å‘å‰è®¡ç®—é‡å éƒ¨åˆ†
        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:
            overlap = sentences[prev] + ' ' + overlap
            prev -= 1
        chunk = overlap+chunk
        next = i + 1
        # å‘åè®¡ç®—å½“å‰chunk
        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:
            chunk = chunk + ' ' + sentences[next]
            next += 1
        chunks.append(chunk)
        i = next
    return chunks
```

<div class="alert alert-info">
æ­¤å¤„ sent_tokenize ä¸ºé’ˆå¯¹è‹±æ–‡çš„å®ç°ï¼Œé’ˆå¯¹ä¸­æ–‡çš„å®ç°è¯·å‚è€ƒ chinese_utils.py
</div>


```python
chunks = split_text(paragraphs, 300, 100)
```


```python
# åˆ›å»ºä¸€ä¸ªå‘é‡æ•°æ®åº“å¯¹è±¡
vector_db = MyVectorDBConnector("demo_text_split", get_embeddings)
# å‘å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–‡æ¡£
vector_db.add_documents(chunks)
# åˆ›å»ºä¸€ä¸ªRAGæœºå™¨äºº
bot = RAG_Bot(
    vector_db,
    llm_api=get_completion
)
```


```python
# user_query = "llama 2å¯ä»¥å•†ç”¨å—ï¼Ÿ"
user_query="llama 2 chatæœ‰å¤šå°‘å‚æ•°"

search_results = vector_db.search(user_query, 2)
for doc in search_results['documents'][0]:
    print(doc+"\n")

response = bot.chat(user_query)
print("====å›å¤====")
print(response)
```

    Llama 2-Chat, a fine-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net benefit to society.
    
    In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.
    
    ====å›å¤====
    Llama 2 Chatæ¨¡å‹æœ‰7Bã€13Bå’Œ70Bå‚æ•°çš„å˜ä½“ã€‚


### 5.2ã€æ£€ç´¢åæ’åºï¼ˆé€‰ï¼‰


**é—®é¢˜**: æœ‰æ—¶ï¼Œæœ€åˆé€‚çš„ç­”æ¡ˆä¸ä¸€å®šæ’åœ¨æ£€ç´¢çš„æœ€å‰é¢



```python
user_query = "how safe is llama 2"
search_results = vector_db.search(user_query, 5)

for doc in search_results['documents'][0]:
    print(doc+"\n")

response = bot.chat(user_query)
print("====å›å¤====")
print(response)
```

    We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).
    
    We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.
    
    In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.
    
    Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial useâ€¡: 1.
    
    We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.
    
    ====å›å¤====
    æ ¹æ®å·²çŸ¥ä¿¡æ¯ï¼Œæˆ‘ä»¬ç›¸ä¿¡åœ¨å®‰å…¨çš„æƒ…å†µä¸‹ï¼Œå…¬å¼€å‘å¸ƒLLMså°†å¯¹ç¤¾ä¼šäº§ç”Ÿå‡€åˆ©ç›Šã€‚åƒæ‰€æœ‰çš„LLMsä¸€æ ·ï¼ŒLlama 2æ˜¯ä¸€é¡¹æ–°æŠ€æœ¯ï¼Œåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­å­˜åœ¨æ½œåœ¨é£é™©ã€‚ç„¶è€Œï¼Œå…³äºLlama 2çš„å®‰å…¨æ€§å…·ä½“ä¿¡æ¯å¹¶æœªæåŠã€‚å› æ­¤ï¼Œæ ¹æ®å·²çŸ¥ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚


**æ–¹æ¡ˆ**:

1. æ£€ç´¢æ—¶è¿‡æ‹›å›ä¸€éƒ¨åˆ†æ–‡æœ¬
2. é€šè¿‡ä¸€ä¸ªæ’åºæ¨¡å‹å¯¹ query å’Œ document é‡æ–°æ‰“åˆ†æ’åº

<img src="_images/llm/sbert-rerank.png" style="margin-left: 0px" width=600px>


<div class="alert alert-danger">
ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼å¯ä¸‹è½½å·¦ä¾§ rank.py åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚
</div>

<div class="alert alert-warning">
<b>å¤‡æ³¨ï¼š</b>
<div>ç”±äº huggingface è¢«å¢™ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†æœ¬ç« ç›¸å…³æ¨¡å‹ã€‚è¯·ç‚¹å‡»ä»¥ä¸‹ç½‘ç›˜é“¾æ¥è¿›è¡Œä¸‹è½½ï¼š

é“¾æ¥: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y æå–ç : 3v6y </div>
</div>



```python
!pip install sentence_transformers
```


```python
from sentence_transformers import CrossEncoder

model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)
```


```python
user_query = "how safe is llama 2"

scores = model.predict([(user_query, doc)
                       for doc in search_results['documents'][0]])
# æŒ‰å¾—åˆ†æ’åº
sorted_list = sorted(
    zip(scores, search_results['documents'][0]), key=lambda x: x[0], reverse=True)
for score, doc in sorted_list:
    print(f"{score}\t{doc}\n")
```

    6.470586776733398	We believe that the open release of LLMs, when done safely, will be a net benefit to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).
    
    5.3834547996521	In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.
    
    4.7099528312683105	We provide a responsible use guideÂ¶ and code examplesâ€– to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.
    
    4.543964862823486	We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.
    
    4.033888339996338	Additionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial useâ€¡: 1.


â€‹    

## å…­ã€å‘é‡æ¨¡å‹çš„æœ¬åœ°éƒ¨ç½²


<div class="alert alert-danger">
ä»¥ä¸‹ä»£ç ä¸è¦åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä¼šæ­»æœºï¼å¯ä¸‹è½½å·¦ä¾§ bge.py åœ¨è‡ªå·±æœ¬åœ°è¿è¡Œã€‚
</div>

<div class="alert alert-warning">
<b>å¤‡æ³¨ï¼š</b>
<div>ç”±äº huggingface è¢«å¢™ï¼Œæˆ‘ä»¬å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†æœ¬ç« ç›¸å…³æ¨¡å‹ã€‚è¯·ç‚¹å‡»ä»¥ä¸‹ç½‘ç›˜é“¾æ¥è¿›è¡Œä¸‹è½½ï¼š

é“¾æ¥: https://pan.baidu.com/s/1X0kfNKasvWqCLUEEyAvO-Q?pwd=3v6y æå–ç : 3v6y </div>
</div>


```python
from sentence_transformers import SentenceTransformer

model_name = 'BAAI/bge-large-zh-v1.5' #ä¸­æ–‡
#model_name = 'moka-ai/m3e-base' #ä¸­è‹±åŒè¯­ï¼Œä½†æ•ˆæœä¸€èˆ¬

model = SentenceTransformer(model_name)
```


```python
query = "å›½é™…äº‰ç«¯"
#query = "global conflicts"

documents = [
    "è”åˆå›½å°±è‹ä¸¹è¾¾å°”å¯Œå°”åœ°åŒºå¤§è§„æ¨¡æš´åŠ›äº‹ä»¶å‘å‡ºè­¦å‘Š",
    "åœŸè€³å…¶ã€èŠ¬å…°ã€ç‘å…¸ä¸åŒ—çº¦ä»£è¡¨å°†ç»§ç»­å°±ç‘å…¸â€œå…¥çº¦â€é—®é¢˜è¿›è¡Œè°ˆåˆ¤",
    "æ—¥æœ¬å²é˜œå¸‚é™†ä¸Šè‡ªå«é˜Ÿå°„å‡»åœºå†…å‘ç”Ÿæªå‡»äº‹ä»¶ 3äººå—ä¼¤",
    "å›½å®¶æ¸¸æ³³ä¸­å¿ƒï¼ˆæ°´ç«‹æ–¹ï¼‰ï¼šæ¢å¤æ¸¸æ³³ã€å¬‰æ°´ä¹å›­ç­‰æ°´ä¸Šé¡¹ç›®è¿è¥",
    "æˆ‘å›½é¦–æ¬¡åœ¨ç©ºé—´ç«™å¼€å±•èˆ±å¤–è¾å°„ç”Ÿç‰©å­¦æš´éœ²å®éªŒ",
]

query_vec = model.encode(query)

doc_vecs = [
    model.encode(doc)
    for doc in documents
]

print("Cosine distance:")  # è¶Šå¤§è¶Šç›¸ä¼¼
#print(cos_sim(query_vec, query_vec))
for vec in doc_vecs:
    print(cos_sim(query_vec, vec))
```

    Cosine distance:
    0.6958812
    0.65735227
    0.6653426
    0.6371888
    0.6942898


<div class="alert alert-info">
<b>æ‰©å±•é˜…è¯»ï¼šhttps://github.com/FlagOpen/FlagEmbedding</b>
</div>


<div class="alert alert-success">
<b>åˆ’é‡ç‚¹ï¼š</b>
    <ol>
        <li>ä¸æ˜¯æ¯ä¸ª Embedding æ¨¡å‹éƒ½å¯¹ä½™å¼¦è·ç¦»å’Œæ¬§æ°è·ç¦»åŒæ—¶æœ‰æ•ˆ</li>
        <li>å“ªç§ç›¸ä¼¼åº¦è®¡ç®—æœ‰æ•ˆè¦é˜…è¯»æ¨¡å‹çš„è¯´æ˜ï¼ˆé€šå¸¸éƒ½æ”¯æŒä½™å¼¦è·ç¦»è®¡ç®—ï¼‰</li>
    </ol>
</div>


## OpenAI Assistants API å†…ç½®äº†è¿™ä¸ªèƒ½åŠ›



```python
from openai import OpenAI # éœ€è¦1.2ä»¥ä¸Šç‰ˆæœ¬
import os
# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv())

client = OpenAI() # openai >= 1.3.0 èµ·ï¼ŒOPENAI_API_KEY å’Œ OPENAI_BASE_URL ä¼šè¢«é»˜è®¤ä½¿ç”¨

# ä¸Šä¼ æ–‡ä»¶
file = client.files.create(
  file=open("llama2.pdf", "rb"),
  purpose='assistants'
)

# åˆ›å»º Assistant
assistant = client.beta.assistants.create(
  instructions="ä½ æ˜¯ä¸ªé—®ç­”æœºå™¨äººï¼Œä½ æ ¹æ®ç»™å®šçš„çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚",
  model="gpt-4-1106-preview",
  tools=[{"type": "retrieval"}],
  file_ids=[file.id]
)

# åˆ›å»º Thread
thread = client.beta.threads.create()

# åˆ›å»º User Message
message = client.beta.threads.messages.create(
    thread_id=thread.id,
    role="user",
    content="Llama 2æœ‰å¤šå°‘å‚æ•°"
)

# åˆ›å»º Run å®ä¾‹ï¼ŒåŒæ—¶ç»™ Assistant æä¾›æŒ‡ä»¤
run = client.beta.threads.runs.create(
  thread_id=thread.id,
  assistant_id=assistant.id,
  instructions="è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚",
)

# ç­‰å¾… Run å®Œæˆ
while run.status not in ["cancelled", "failed", "completed", "expired"]:
    run = client.beta.threads.runs.retrieve(
      thread_id=thread.id,
      run_id=run.id
    )

# è·å– Run çš„ç»“æœ
messages = client.beta.threads.messages.list(
  thread_id=thread.id
)

# æ‰“å°ç»“æœ
for turn in reversed(messages.data):
    print(f"{turn.role.upper()}: "+turn.content[0].text.value)

```

    USER: Llama 2æœ‰å¤šå°‘å‚æ•°
    ASSISTANT: Llama 2æœ‰ä¸‰ç§ä¸åŒå‚æ•°è§„æ¨¡çš„å˜ç§ï¼Œåˆ†åˆ«æ˜¯7Bï¼ˆ70äº¿ï¼‰ï¼Œ13Bï¼ˆ130äº¿ï¼‰å’Œ70Bï¼ˆ700äº¿ï¼‰å‚æ•°ã€9â€ sourceã€‘ã€‚


### OpenAI æ˜¯æ€ä¹ˆå®ç°çš„

<img src="_images/llm/assistant_api_retrieval.png" style="margin-left: 0px" width="800px">

https://platform.openai.com/docs/assistants/tools/knowledge-retrieval


<div class="alert alert-warning">
<b>æˆ‘ä»¬ä¸ºä»€ä¹ˆä»ç„¶éœ€è¦äº†è§£æ•´ä¸ªå®ç°è¿‡ç¨‹ï¼Ÿ</b>
<ol>
<li>å¦‚æœä¸èƒ½ä½¿ç”¨ OpenAIï¼Œè¿˜æ˜¯éœ€è¦æ‰‹å·¥å®ç° RAG æµç¨‹</li>
<li>äº†è§£ RAG çš„åŸç†ï¼Œå¯ä»¥æŒ‡å¯¼ä½ çš„äº§å“å¼€å‘ï¼ˆå›å¿† GitHub Copilotï¼‰</li>
<li>ç”¨ç§æœ‰çŸ¥è¯†å¢å¼º LLM çš„èƒ½åŠ›ï¼Œæ˜¯ä¸€ä¸ªé€šç”¨çš„æ–¹æ³•è®º</li>
</div>


## æ€»ç»“

### RAG çš„æµç¨‹

- ç¦»çº¿æ­¥éª¤ï¼š
  1. æ–‡æ¡£åŠ è½½
  2. æ–‡æ¡£åˆ‡åˆ†
  3. å‘é‡åŒ–
  4. çŒå…¥å‘é‡æ•°æ®åº“
  
- åœ¨çº¿æ­¥éª¤ï¼š
  1. è·å¾—ç”¨æˆ·é—®é¢˜
  2. ç”¨æˆ·é—®é¢˜å‘é‡åŒ–
  3. æ£€ç´¢å‘é‡æ•°æ®åº“
  4. å°†æ£€ç´¢ç»“æœå’Œç”¨æˆ·é—®é¢˜å¡«å…¥ Prompt æ¨¡ç‰ˆ
  5. ç”¨æœ€ç»ˆè·å¾—çš„ Prompt è°ƒç”¨ LLM
  6. ç”± LLM ç”Ÿæˆå›å¤

### æˆ‘ç”¨äº†ä¸€ä¸ªå¼€æºçš„ RAGï¼Œä¸å¥½ä½¿æ€ä¹ˆåŠï¼Ÿ

1. æ£€æŸ¥é¢„å¤„ç†æ•ˆæœï¼šæ–‡æ¡£åŠ è½½æ˜¯å¦æ­£ç¡®ï¼Œåˆ‡å‰²çš„æ˜¯å¦åˆç†
2. æµ‹è¯•æ£€ç´¢æ•ˆæœï¼šé—®é¢˜æ£€ç´¢å›æ¥çš„æ–‡æœ¬ç‰‡æ®µæ˜¯å¦åŒ…å«ç­”æ¡ˆ
3. æµ‹è¯•å¤§æ¨¡å‹èƒ½åŠ›ï¼šç»™å®šé—®é¢˜å’ŒåŒ…å«ç­”æ¡ˆæ–‡æœ¬ç‰‡æ®µçš„å‰æä¸‹ï¼Œå¤§æ¨¡å‹èƒ½ä¸èƒ½æ­£ç¡®å›ç­”é—®é¢˜


## ä½œä¸š

åšä¸ªè‡ªå·±çš„ ChatPDFã€‚éœ€æ±‚ï¼š

1. ä»æœ¬åœ°åŠ è½½ PDF æ–‡ä»¶ï¼ŒåŸºäº PDF çš„å†…å®¹å¯¹è¯
2. å¯ä»¥æ— å‰ç«¯ï¼Œåªè¦èƒ½åœ¨å‘½ä»¤è¡Œè¿è¡Œå°±è¡Œ
3. å…¶å®ƒéšæ„å‘æŒ¥

